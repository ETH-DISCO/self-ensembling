Reproducing: "Ensemble everything everywhere: Multi-scale aggregation for adversarial robustness" (https://arxiv.org/pdf/2408.05446)

Based on @aplesner's implementation: https://github.com/aplesner/Ensemble-everything-everywhere-recreating-results

# usage

```bash
...
```

# notes

figures 5 and 6 in the paper show outputs from 54 layers. we only ended up with 53 layers, namely:

- input layer
- conv1 + batchnorm1 + relu + maxpool (combined as one layer)
- 50 layers after each block (see: `resnet._make_layer`)
- average pooling layer
