{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "assert gpu_memory >= 79, \"at least 80GB gpu memory required\"\n",
    "set_env()\n",
    "\n",
    "data_path = get_current_dir().parent / \"data\"\n",
    "dataset_path = get_current_dir().parent / \"datasets\"\n",
    "weights_path = get_current_dir().parent / \"weights\"\n",
    "output_path = get_current_dir()\n",
    "\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "os.makedirs(weights_path, exist_ok=True)\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "prerendered_mask = Image.open(get_current_dir() / \"masks\" / \"mask.png\")\n",
    "\n",
    "\n",
    "def get_dataset(dataset: str):\n",
    "    if dataset == \"cifar10\":\n",
    "        num_classes = 10\n",
    "        trainset = torchvision.datasets.CIFAR10(root=dataset_path, train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "        testset = torchvision.datasets.CIFAR10(root=dataset_path, train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "        original_images_train_np = np.array(trainset.data)\n",
    "        original_labels_train_np = np.array(trainset.targets)\n",
    "        original_images_test_np = np.array(testset.data)\n",
    "        original_labels_test_np = np.array(testset.targets)\n",
    "    elif dataset == \"cifar100\":\n",
    "        num_classes = 100\n",
    "        trainset = torchvision.datasets.CIFAR100(root=dataset_path, train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "        testset = torchvision.datasets.CIFAR100(root=dataset_path, train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "        original_images_train_np = np.array(trainset.data)\n",
    "        original_labels_train_np = np.array(trainset.targets)\n",
    "        original_images_test_np = np.array(testset.data)\n",
    "        original_labels_test_np = np.array(testset.targets)\n",
    "    elif dataset == \"imagenette\":\n",
    "        num_classes = 10\n",
    "        imgpath = dataset_path / \"imagenette2\"\n",
    "        transform = torchvision.transforms.Compose([torchvision.transforms.Resize((224, 224)), torchvision.transforms.ToTensor()])\n",
    "        trainset = torchvision.datasets.Imagenette(root=dataset_path, split=\"train\", download=(not (imgpath / \"train\").exists()), transform=transform)\n",
    "        testset = torchvision.datasets.Imagenette(root=dataset_path, split=\"val\", download=(not (imgpath / \"val\").exists()), transform=transform)\n",
    "        num_train = len(trainset)\n",
    "        num_test = len(testset)\n",
    "        original_images_train_np = np.empty((num_train, 224, 224, 3), dtype=np.uint8)\n",
    "        original_labels_train_np = np.empty(num_train, dtype=np.int64)\n",
    "        original_images_test_np = np.empty((num_test, 224, 224, 3), dtype=np.uint8)\n",
    "        original_labels_test_np = np.empty(num_test, dtype=np.int64)\n",
    "        for idx, (image, label) in tqdm(enumerate(trainset), total=num_train, desc=\"preprocessing trainset\", ncols=100):\n",
    "            original_images_train_np[idx] = (np.transpose(image.numpy(), (1, 2, 0)) * 255).astype(np.uint8)\n",
    "            original_labels_train_np[idx] = label\n",
    "        for idx, (image, label) in tqdm(enumerate(testset), total=num_test, desc=\"preprocessing testset\", ncols=100):\n",
    "            original_images_test_np[idx] = (np.transpose(image.numpy(), (1, 2, 0)) * 255).astype(np.uint8)\n",
    "            original_labels_test_np[idx] = label\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "    images_train_np = original_images_train_np / 255.0\n",
    "    images_test_np = original_images_test_np / 255.0\n",
    "    labels_train_np = original_labels_train_np\n",
    "    labels_test_np = original_labels_test_np\n",
    "    return images_train_np, labels_train_np, images_test_np, labels_test_np, num_classes\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "training phase\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def make_multichannel_input(images, enable_noise, enable_random_shuffle, resolutions):\n",
    "    down_noise = 0.2  # noise standard deviation to be added at the low resolution\n",
    "    up_noise = 0.2  # noise stadard deviation to be added at the high resolution\n",
    "    jit_size = 3  # max size of the x-y jit in each axis, sampled uniformly from -jit_size to +jit_size inclusive\n",
    "    up_res = 32  # hard coded for CIFAR-10 or CIFAR-100  <---------------- THIS IS WHY IMAGENETTE ISN'T WORKING\n",
    "\n",
    "    if not enable_noise:\n",
    "        return torch.concatenate([images] * len(resolutions), axis=1)  # don't do anything\n",
    "\n",
    "    all_channels = []\n",
    "    for i, r in enumerate(resolutions):\n",
    "\n",
    "        def apply_transformations(images, down_res, up_res, jit_x, jit_y, down_noise, up_noise, contrast, color_amount):\n",
    "            # images = torch.mean(images, axis=1, keepdims=True) # only for mnist\n",
    "            images_collected = []\n",
    "            for i in range(images.shape[0]):\n",
    "                image = images[i]\n",
    "                image = torchvision.transforms.functional.adjust_contrast(image, contrast[i])  # changing contrast\n",
    "                image = torch.roll(image, shifts=(jit_x[i], jit_y[i]), dims=(-2, -1))  # shift the result in x and y\n",
    "                image = color_amount[i] * image + torch.mean(image, axis=0, keepdims=True) * (1 - color_amount[i])  # shifting in the color <-> grayscale axis\n",
    "                images_collected.append(image)\n",
    "            images = torch.stack(images_collected, axis=0)\n",
    "            images = F.interpolate(images, size=(down_res, down_res), mode=\"bicubic\")  # descrease the resolution\n",
    "            noise = down_noise * torch.Tensor(np.random.rand(images.shape[0], 3, down_res, down_res)).to(\"cuda\")  # low res noise\n",
    "            images = images + noise\n",
    "            images = F.interpolate(images, size=(up_res, up_res), mode=\"bicubic\")  # increase the resolution\n",
    "            noise = up_noise * torch.Tensor(np.random.rand(images.shape[0], 3, up_res, up_res)).to(\"cuda\")  # high res noise\n",
    "            images = images + noise\n",
    "            images = torch.clip(images, 0, 1)  # clipping to the right range of values\n",
    "            return images\n",
    "\n",
    "        images_now = apply_transformations(\n",
    "            images,\n",
    "            down_res=r,\n",
    "            up_res=up_res,\n",
    "            jit_x=np.random.choice(range(-jit_size, jit_size + 1), len(images + i)),  # x-shift,\n",
    "            jit_y=np.random.choice(range(-jit_size, jit_size + 1), len(51 * images + 7 * i + 125 * r)),  # y-shift\n",
    "            down_noise=down_noise,\n",
    "            up_noise=up_noise,\n",
    "            contrast=np.random.choice(np.linspace(0.7, 1.5, 100), len(7 + 3 * images + 9 * i + 5 * r)),  # change in contrast,\n",
    "            color_amount=np.random.choice(np.linspace(0.5, 1.0, 100), len(5 + 7 * images + 8 * i + 2 * r)),  # change in color amount\n",
    "        )\n",
    "        all_channels.append(images_now)\n",
    "\n",
    "    if not enable_random_shuffle:\n",
    "        return torch.concatenate(all_channels, axis=1)\n",
    "    elif enable_random_shuffle:\n",
    "        indices = torch.randperm(len(all_channels))\n",
    "        shuffled_tensor_list = [all_channels[i] for i in indices]\n",
    "        return torch.concatenate(shuffled_tensor_list, axis=1)\n",
    "\n",
    "\n",
    "def eval_model(model, images_in, labels_in, batch_size=128):\n",
    "    all_preds = []\n",
    "    all_logits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        its = int(np.ceil(float(len(images_in)) / float(batch_size)))\n",
    "        pbar = tqdm(range(its), desc=\"eval\", ncols=100)\n",
    "        for it in pbar:\n",
    "            i1 = it * batch_size\n",
    "            i2 = min([(it + 1) * batch_size, len(images_in)])\n",
    "\n",
    "            inputs = torch.Tensor(images_in[i1:i2].transpose([0, 3, 1, 2])).to(\"cuda\")\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            all_logits.append(outputs.detach().cpu().numpy())\n",
    "            preds = torch.argmax(outputs, axis=-1)\n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_logits = np.concatenate(all_logits, axis=0)\n",
    "    return np.sum(all_preds == labels_in), all_preds.shape[0], all_logits  # hits, count, logits\n",
    "\n",
    "\n",
    "def train_model(model_in, images_in, labels_in, epochs, lr, batch_size, optimizer_in, subset_only, mode, use_adversarial_training, adversarial_epsilon, skip_test_set_eval):\n",
    "    def fgsm_attack(model, xs, ys, epsilon, random_reps, batch_size):\n",
    "        model = model.eval()\n",
    "        model = model.cuda()\n",
    "\n",
    "        all_perturbed_images = []\n",
    "\n",
    "        its = int(np.ceil(xs.shape[0] / batch_size))\n",
    "        for it in range(its):\n",
    "            i1 = it * batch_size\n",
    "            i2 = min([(it + 1) * batch_size, xs.shape[0]])\n",
    "\n",
    "            x = torch.Tensor(xs[i1:i2].transpose([0, 3, 1, 2])).to(\"cuda\")\n",
    "            y = torch.Tensor(ys[i1:i2]).to(\"cuda\").to(torch.long)\n",
    "            x.requires_grad = True\n",
    "\n",
    "            for _ in range(random_reps):\n",
    "                outputs = model(x)\n",
    "                loss = nn.CrossEntropyLoss()(outputs, y)\n",
    "                loss.backward()\n",
    "\n",
    "            perturbed_image = x + epsilon * x.grad.data.sign()\n",
    "            perturbed_image = torch.clip(perturbed_image, 0, 1)\n",
    "            all_perturbed_images.append(perturbed_image.detach().cpu().numpy().transpose([0, 2, 3, 1]))\n",
    "        return np.concatenate(all_perturbed_images, axis=0)\n",
    "\n",
    "    global storing_models\n",
    "\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "\n",
    "    if mode == \"train\":\n",
    "        model_in.train()\n",
    "    elif mode == \"eval\":\n",
    "        model_in.eval()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if subset_only is None:\n",
    "        train_optimizer = optimizer_in(model_in.parameters(), lr=lr)\n",
    "    else:\n",
    "        train_optimizer = optimizer_in(subset_only, lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"epoch={epoch}/{epochs}\")\n",
    "        if mode == \"train\":  # avoid random flips\n",
    "            model_in.train()\n",
    "        elif mode == \"eval\":\n",
    "            model_in.eval()\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        all_hits = []\n",
    "\n",
    "        randomized_ids = np.random.permutation(range(len(images_in)))\n",
    "        its = int(np.ceil(float(len(images_in)) / float(batch_size)))\n",
    "        pbar = tqdm(range(its), desc=\"Training\", ncols=100)\n",
    "        for it in pbar:\n",
    "            i1 = it * batch_size\n",
    "            i2 = min([(it + 1) * batch_size, len(images_in)])\n",
    "\n",
    "            np_images_used = images_in[randomized_ids[i1:i2]]\n",
    "            np_labels_used = labels_in[randomized_ids[i1:i2]]\n",
    "\n",
    "            if use_adversarial_training:  # very light adversarial training if on\n",
    "                attacked_images = fgsm_attack(model_in.eval(), np_images_used[:], np_labels_used[:], epsilon=adversarial_epsilon, random_reps=1, batch_size=batch_size // 2)\n",
    "                np_images_used = attacked_images\n",
    "                np_labels_used = np_labels_used\n",
    "                if mode == \"train\":\n",
    "                    model_in.train()\n",
    "                elif mode == \"eval\":\n",
    "                    model_in.eval()\n",
    "\n",
    "            inputs = torch.Tensor(np_images_used.transpose([0, 3, 1, 2])).to(\"cuda\")\n",
    "            labels = torch.Tensor(np_labels_used).to(\"cuda\").to(torch.long)\n",
    "\n",
    "            # the actual optimization step\n",
    "            train_optimizer.zero_grad()\n",
    "            outputs = model_in(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            train_optimizer.step()\n",
    "\n",
    "            # on the fly eval for the train set batches\n",
    "            preds = torch.argmax(outputs, axis=-1)\n",
    "            acc = torch.mean((preds == labels).to(torch.float), axis=-1)\n",
    "            all_hits.append((preds == labels).to(torch.float).detach().cpu().numpy())\n",
    "            train_accs.append(acc.detach().cpu().numpy())\n",
    "\n",
    "            pbar.set_description(f\"train acc={acc.detach().cpu().numpy()} loss={loss.item()}\")\n",
    "\n",
    "        if not skip_test_set_eval:\n",
    "            with isolated_environment():\n",
    "                eval_model_copy = copy.deepcopy(model_in)\n",
    "                test_hits, test_count, _ = eval_model(eval_model_copy.eval(), images_test_np, labels_test_np)\n",
    "\n",
    "        # end of epoch eval\n",
    "        train_hits = np.sum(np.concatenate(all_hits, axis=0).reshape([-1]))\n",
    "        train_count = np.concatenate(all_hits, axis=0).reshape([-1]).shape[0]\n",
    "        print(f\"e={epoch} train {train_hits} / {train_count} = {train_hits/train_count},  test {test_hits} / {test_count} = {test_hits/test_count}\")\n",
    "\n",
    "        test_accs.append((test_hits / test_count) if (test_count > 0) else 0)\n",
    "    return model_in, train_accs, test_accs\n",
    "\n",
    "\n",
    "class WrapModelForResNet152(torch.nn.Module):\n",
    "    def __init__(self, model, multichannel_fn, num_classes=10):\n",
    "        super(WrapModelForResNet152, self).__init__()\n",
    "        self.multichannel_fn = multichannel_fn  # multi-res input\n",
    "        self.model = model\n",
    "        self.num_classes = num_classes\n",
    "        self.layer_operations = [\n",
    "            torch.nn.Sequential(\n",
    "                model.conv1,\n",
    "                model.bn1,\n",
    "                model.relu,\n",
    "                model.maxpool,\n",
    "            ),\n",
    "            *model.layer1,\n",
    "            *model.layer2,\n",
    "            *model.layer3,\n",
    "            *model.layer4,\n",
    "            model.avgpool,\n",
    "            model.fc,\n",
    "        ]\n",
    "        self.all_dims = [\n",
    "            3 * 224 * 224 * len(resolutions),\n",
    "            64 * 56 * 56,\n",
    "            *[256 * 56 * 56] * len(model.layer1),\n",
    "            *[512 * 28 * 28] * len(model.layer2),\n",
    "            *[1024 * 14 * 14] * len(model.layer3),\n",
    "            *[2048 * 7 * 7] * len(model.layer4),\n",
    "            2048,\n",
    "            1000,\n",
    "        ]\n",
    "\n",
    "        class BatchNormLinear(nn.Module):\n",
    "            def __init__(self, in_features, out_features, device=\"cuda\"):\n",
    "                super(BatchNormLinear, self).__init__()\n",
    "                self.batch_norm = nn.BatchNorm1d(in_features, device=device)\n",
    "                self.linear = nn.Linear(in_features, out_features, device=device)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = self.batch_norm(x)\n",
    "                return self.linear(x)\n",
    "\n",
    "        self.linear_layers = torch.nn.ModuleList([BatchNormLinear(self.all_dims[i], num_classes, device=\"cuda\") for i in range(len(self.all_dims))])\n",
    "\n",
    "    def prepare_input(self, x):  # preprocess\n",
    "        x = self.multichannel_fn(x)\n",
    "        x = F.interpolate(x, size=(224, 224), mode=\"bicubic\")\n",
    "        x = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406] * (x.shape[1] // 3), std=[0.229, 0.224, 0.225] * (x.shape[1] // 3))(x)\n",
    "        return x\n",
    "\n",
    "    def forward_until(self, x, layer_id):\n",
    "        x = self.prepare_input(x)\n",
    "        for l in range(layer_id):\n",
    "            if list(x.shape)[1:] == [2048, 1, 1]:\n",
    "                x = x.reshape([-1, 2048])\n",
    "            x = self.layer_operations[l](x)\n",
    "        return x\n",
    "\n",
    "    def forward_after(self, x, layer_id):\n",
    "        x = self.prepare_input(x)\n",
    "        for l in range(layer_id, len(self.layer_operations)):\n",
    "            if list(x.shape)[1:] == [2048, 1, 1]:\n",
    "                x = x.reshape([-1, 2048])\n",
    "            x = self.layer_operations[l](x)\n",
    "        return x\n",
    "\n",
    "    def predict_from_layer(self, x, l):  # called by LinearNet forward\n",
    "        x = self.forward_until(x, l)\n",
    "        x = x.reshape([x.shape[0], -1])\n",
    "        return self.linear_layers[l](x)\n",
    "\n",
    "    def predict_from_several_layers(self, x, layers):\n",
    "        x = self.prepare_input(x)\n",
    "        outputs = dict()\n",
    "        outputs[0] = self.linear_layers[0](x.reshape([x.shape[0], -1]))\n",
    "        for l in range(len(self.layer_operations)):\n",
    "            if list(x.shape)[1:] == [2048, 1, 1]:\n",
    "                x = x.reshape([-1, 2048])\n",
    "            x = self.layer_operations[l](x)\n",
    "            if l in layers:\n",
    "                outputs[l + 1] = self.linear_layers[l + 1](x.reshape([x.shape[0], -1]))\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, model, layer_i):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.model = model\n",
    "        self.layer_i = layer_i\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.model.predict_from_layer(inputs, self.layer_i)\n",
    "\n",
    "\n",
    "def get_model(enable_noise, enable_random_shuffle, enable_adversarial_training, resolutions, layers_to_use, num_classes, images_train_np, labels_train_np):\n",
    "    #\n",
    "    # backbone model\n",
    "    #\n",
    "\n",
    "    from torchvision.models import ResNet152_Weights, resnet152\n",
    "\n",
    "    imported_model = resnet152(weights=ResNet152_Weights.IMAGENET1K_V2)\n",
    "\n",
    "    # replace first conv layer with multi-res one\n",
    "    in_planes = 3\n",
    "    planes = 64\n",
    "    stride = 2\n",
    "    N = len(resolutions)  # input channels multiplier due to multi-res input\n",
    "    conv2 = nn.Conv2d(in_channels=N * in_planes, out_channels=planes, kernel_size=7, stride=stride, padding=3, bias=False)\n",
    "    imported_model.conv1 = copy.deepcopy(conv2)\n",
    "\n",
    "    # set num of classes in final layer\n",
    "    imported_model.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    class ImportedModelWrapper(nn.Module):\n",
    "        def __init__(self, imported_model, multichannel_fn):\n",
    "            super(ImportedModelWrapper, self).__init__()\n",
    "            self.imported_model = imported_model\n",
    "            self.multichannel_fn = multichannel_fn\n",
    "\n",
    "        def forward(self, x):\n",
    "            # multichannel input\n",
    "            x = self.multichannel_fn(x)\n",
    "            # imagenet preprocessing\n",
    "            x = F.interpolate(x, size=(224, 224), mode=\"bicubic\")\n",
    "            x = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406] * (x.shape[1] // 3), std=[0.229, 0.224, 0.225] * (x.shape[1] // 3))(x)\n",
    "            x = self.imported_model(x)\n",
    "            return x\n",
    "\n",
    "    configured_make_multichannel_input = lambda x: make_multichannel_input(x, enable_noise=enable_noise, enable_random_shuffle=enable_random_shuffle, resolutions=resolutions)\n",
    "    wrapped_model = ImportedModelWrapper(imported_model, configured_make_multichannel_input).to(\"cuda\")\n",
    "    wrapped_model.multichannel_fn = configured_make_multichannel_input\n",
    "    model = copy.deepcopy(wrapped_model)\n",
    "    model.multichannel_fn = configured_make_multichannel_input\n",
    "    model.train()\n",
    "\n",
    "    # check cache\n",
    "    args_hash = hashlib.md5(json.dumps({k: v for k, v in locals().items() if isinstance(v, (int, float, str, bool, list, dict))}, sort_keys=True).encode()).hexdigest()\n",
    "    cache_name = f\"tmp_{args_hash}.pth\"\n",
    "    if (weights_path / cache_name).exists():\n",
    "        cached_model = WrapModelForResNet152(imported_model, configured_make_multichannel_input, num_classes=num_classes)\n",
    "        cached_model.load_state_dict(torch.load(weights_path / cache_name, weights_only=True))\n",
    "        print(f\"loaded cached model: {cache_name}\")\n",
    "        return cached_model\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    model, train_accs, test_accs = train_model(\n",
    "        model_in=model,\n",
    "        images_in=images_train_np,\n",
    "        labels_in=labels_train_np,\n",
    "        epochs=6,\n",
    "        lr=3.3e-5,\n",
    "        batch_size=128,\n",
    "        optimizer_in=optim.Adam,\n",
    "        subset_only=None,\n",
    "        mode=\"train\",\n",
    "        use_adversarial_training=enable_adversarial_training,\n",
    "        adversarial_epsilon=8 / 255,\n",
    "        skip_test_set_eval=False,\n",
    "    )\n",
    "    print(f\"trained backbone model - avg train acc: {np.mean(train_accs)}, avg test acc: {np.mean(test_accs)}\")\n",
    "\n",
    "    #\n",
    "    # training linear layers of self-ensemble\n",
    "    #\n",
    "\n",
    "    resnet152_wrapper = WrapModelForResNet152(model.imported_model, configured_make_multichannel_input, num_classes=num_classes)\n",
    "    resnet152_wrapper.multichannel_fn = configured_make_multichannel_input\n",
    "    resnet152_wrapper = resnet152_wrapper.to(\"cuda\")\n",
    "    backbone_model = copy.deepcopy(resnet152_wrapper)\n",
    "    del resnet152_wrapper\n",
    "\n",
    "    backbone_model.eval()\n",
    "    linear_model = LinearNet(backbone_model, 5).to(\"cuda\")  # preallocate\n",
    "    linear_layers_collected_dict = dict()\n",
    "\n",
    "    for layer_i in reversed(layers_to_use):\n",
    "        linear_model.layer_i = layer_i\n",
    "        linear_model.fixed_mode = \"train\"\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        linear_model, train_accs, test_accs = train_model(\n",
    "            model_in=linear_model,\n",
    "            images_in=images_train_np[:],\n",
    "            labels_in=labels_train_np[:],\n",
    "            epochs=1,\n",
    "            lr=3.3e-5,\n",
    "            batch_size=64,\n",
    "            optimizer_in=optim.Adam,\n",
    "            subset_only=linear_model.model.linear_layers[layer_i].parameters(),  # just the linear projection\n",
    "            mode=\"train\",\n",
    "            use_adversarial_training=False,\n",
    "            adversarial_epsilon=8 / 255,\n",
    "            skip_test_set_eval=False,\n",
    "        )\n",
    "        linear_layers_collected_dict[layer_i] = copy.deepcopy(backbone_model.linear_layers[layer_i])\n",
    "        print(f\"trained layer={layer_i} - avg train acc: {np.mean(train_accs)}, avg test acc: {np.mean(test_accs)}\")\n",
    "\n",
    "    # copy dict back to backbone\n",
    "    for layer_i in layers_to_use:\n",
    "        backbone_model.linear_layers[layer_i] = copy.deepcopy(linear_layers_collected_dict[layer_i])\n",
    "    del linear_layers_collected_dict\n",
    "\n",
    "    torch.save(backbone_model.state_dict(), weights_path / cache_name)\n",
    "    print(f\"cached model {cache_name} ({sum(p.numel() for p in backbone_model.parameters()) / 1e6:.2f} MB)\")\n",
    "    return backbone_model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "eval phase\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def fgsm_attack_layer(model, xs, ys, epsilon, layer_i, batch_size=128):\n",
    "    model = model.eval()\n",
    "    model = model.cuda()\n",
    "\n",
    "    all_perturbed_images = []\n",
    "    its = int(np.ceil(xs.shape[0] / batch_size))\n",
    "\n",
    "    for it in range(its):\n",
    "        i1 = it * batch_size\n",
    "        i2 = min([(it + 1) * batch_size, xs.shape[0]])\n",
    "\n",
    "        x = torch.Tensor(xs[i1:i2].transpose([0, 3, 1, 2])).to(\"cuda\")\n",
    "        y = torch.Tensor(ys[i1:i2]).to(\"cuda\").to(torch.long)\n",
    "        x.requires_grad = True\n",
    "\n",
    "        # mod: dropped the random_reps\n",
    "        layer_output = model.forward_until(x, layer_i)\n",
    "        layer_logits = model.linear_layers[layer_i](layer_output.reshape(layer_output.shape[0], -1))\n",
    "        loss = nn.CrossEntropyLoss()(layer_logits, y)\n",
    "        loss.backward()\n",
    "\n",
    "        perturbed_image = x + epsilon * x.grad.data.sign()\n",
    "        perturbed_image = torch.clip(perturbed_image, 0, 1)\n",
    "        all_perturbed_images.append(perturbed_image.detach().cpu().numpy().transpose([0, 2, 3, 1]))\n",
    "    return np.concatenate(all_perturbed_images, axis=0)\n",
    "\n",
    "\n",
    "def pgd_attack_layer(model, xs, ys, epsilon, layer_i, alpha=0.01, num_iter=40, batch_size=128):\n",
    "    model = model.eval()\n",
    "    model = model.cuda()\n",
    "\n",
    "    all_perturbed_images = []\n",
    "    its = int(np.ceil(xs.shape[0] / batch_size))\n",
    "\n",
    "    for it in range(its):\n",
    "        i1 = it * batch_size\n",
    "        i2 = min([(it + 1) * batch_size, xs.shape[0]])\n",
    "\n",
    "        x = torch.Tensor(xs[i1:i2].transpose([0, 3, 1, 2])).to(\"cuda\")\n",
    "        y = torch.Tensor(ys[i1:i2]).to(\"cuda\").to(torch.long)\n",
    "        \n",
    "        # initialize delta (perturbation) randomly within epsilon ball\n",
    "        delta = torch.zeros_like(x, requires_grad=True).to(\"cuda\")\n",
    "        delta.uniform_(-epsilon, epsilon)\n",
    "        delta = torch.clamp(x + delta, 0, 1) - x\n",
    "        \n",
    "        for _ in range(num_iter):\n",
    "            x_adv = x + delta\n",
    "            x_adv.requires_grad = True\n",
    "\n",
    "            layer_output = model.forward_until(x_adv, layer_i)\n",
    "            layer_logits = model.linear_layers[layer_i](layer_output.reshape(layer_output.shape[0], -1))\n",
    "            loss = nn.CrossEntropyLoss()(layer_logits, y)\n",
    "            loss.backward()\n",
    "\n",
    "            # update delta with gradient descent\n",
    "            grad = x_adv.grad.data\n",
    "            delta = delta + alpha * grad.sign()\n",
    "            \n",
    "            # project perturbation back onto epsilon ball\n",
    "            delta = torch.clamp(delta, -epsilon, epsilon)\n",
    "            # project perturbed image back into valid range [0,1]\n",
    "            delta = torch.clamp(x + delta, 0, 1) - x\n",
    "            \n",
    "            delta = delta.detach()\n",
    "            delta.requires_grad = True\n",
    "\n",
    "        perturbed_image = torch.clamp(x + delta, 0, 1)\n",
    "        all_perturbed_images.append(perturbed_image.detach().cpu().numpy().transpose([0, 2, 3, 1]))\n",
    "    return np.concatenate(all_perturbed_images, axis=0)\n",
    "\n",
    "\n",
    "def fgsm_attack_layer_combined(model, xs, ys, epsilon, layer_idxs, layer_weights, batch_size=128):\n",
    "    if layer_weights is None:\n",
    "        layer_weights = [1.0 / len(layer_idxs)] * len(layer_idxs)  # equal weights\n",
    "    layer_weights = np.array(layer_weights)\n",
    "    layer_weights = layer_weights / np.sum(layer_weights)  # normalize to sum to 1\n",
    "\n",
    "    all_perturbed_images = []\n",
    "    its = int(np.ceil(xs.shape[0] / batch_size))\n",
    "\n",
    "    for it in range(its):\n",
    "        i1 = it * batch_size\n",
    "        i2 = min([(it + 1) * batch_size, xs.shape[0]])\n",
    "\n",
    "        x = torch.Tensor(xs[i1:i2].transpose([0, 3, 1, 2])).to(\"cuda\")\n",
    "        y = torch.Tensor(ys[i1:i2]).to(\"cuda\").to(torch.long)\n",
    "\n",
    "        combined_grad = torch.zeros_like(x)\n",
    "        for layer_idx, weight in zip(layer_idxs, layer_weights):\n",
    "            x_copy = x.clone()\n",
    "            x_copy.requires_grad = True\n",
    "\n",
    "            layer_output = model.forward_until(x_copy, layer_idx)\n",
    "            layer_logits = model.linear_layers[layer_idx](layer_output.reshape(layer_output.shape[0], -1))\n",
    "            loss = nn.CrossEntropyLoss()(layer_logits, y)\n",
    "            loss.backward()\n",
    "\n",
    "            combined_grad += weight * x_copy.grad.data  # sum from all layers\n",
    "\n",
    "        perturbed_image = x + epsilon * combined_grad.sign()\n",
    "        perturbed_image = torch.clip(perturbed_image, 0, 1)\n",
    "\n",
    "        all_perturbed_images.append(perturbed_image.detach().cpu().numpy().transpose([0, 2, 3, 1]))\n",
    "\n",
    "    return np.concatenate(all_perturbed_images, axis=0)\n",
    "\n",
    "\n",
    "def pgd_attack_layer_combined(model, xs, ys, epsilon, layer_idxs, layer_weights, alpha=0.01, num_iter=40, batch_size=128):\n",
    "    if layer_weights is None:\n",
    "        layer_weights = [1.0 / len(layer_idxs)] * len(layer_idxs)  # equal weights\n",
    "    layer_weights = np.array(layer_weights)\n",
    "    layer_weights = layer_weights / np.sum(layer_weights)  # normalize to sum to 1\n",
    "\n",
    "    all_perturbed_images = []\n",
    "    its = int(np.ceil(xs.shape[0] / batch_size))\n",
    "\n",
    "    for it in range(its):\n",
    "        i1 = it * batch_size\n",
    "        i2 = min([(it + 1) * batch_size, xs.shape[0]])\n",
    "\n",
    "        x = torch.Tensor(xs[i1:i2].transpose([0, 3, 1, 2])).to(\"cuda\")\n",
    "        y = torch.Tensor(ys[i1:i2]).to(\"cuda\").to(torch.long)\n",
    "\n",
    "        delta = torch.zeros_like(x, requires_grad=True).to(\"cuda\")\n",
    "        delta.uniform_(-epsilon, epsilon)\n",
    "        delta = torch.clamp(x + delta, 0, 1) - x\n",
    "\n",
    "        for _ in range(num_iter):\n",
    "            x_adv = x + delta\n",
    "            combined_grad = torch.zeros_like(x)\n",
    "\n",
    "            for layer_idx, weight in zip(layer_idxs, layer_weights):\n",
    "                x_copy = x_adv.clone()\n",
    "                x_copy.requires_grad = True\n",
    "\n",
    "                layer_output = model.forward_until(x_copy, layer_idx)\n",
    "                layer_logits = model.linear_layers[layer_idx](layer_output.reshape(layer_output.shape[0], -1))\n",
    "                loss = nn.CrossEntropyLoss()(layer_logits, y)\n",
    "                loss.backward()\n",
    "\n",
    "                combined_grad += weight * x_copy.grad.data # sum from all layers\n",
    "\n",
    "            delta = delta + alpha * combined_grad.sign()\n",
    "            delta = torch.clamp(delta, -epsilon, epsilon)\n",
    "            delta = torch.clamp(x + delta, 0, 1) - x\n",
    "            \n",
    "            delta = delta.detach()\n",
    "            delta.requires_grad = True\n",
    "\n",
    "        perturbed_image = torch.clamp(x + delta, 0, 1)\n",
    "        all_perturbed_images.append(perturbed_image.detach().cpu().numpy().transpose([0, 2, 3, 1]))\n",
    "\n",
    "    return np.concatenate(all_perturbed_images, axis=0)\n",
    "\n",
    "\n",
    "def fgsm_attack_ensemble(model, images, labels, epsilon, batch_size):\n",
    "    def get_cross_max_consensus_logits(outputs: torch.Tensor, k: int) -> torch.Tensor:\n",
    "        Z_hat = outputs - outputs.max(dim=2, keepdim=True)[0]\n",
    "        Z_hat = Z_hat - Z_hat.max(dim=1, keepdim=True)[0]\n",
    "        Y, _ = torch.topk(Z_hat, k, dim=1)\n",
    "        Y = Y[:, -1, :]\n",
    "        return Y\n",
    "\n",
    "    model = model.eval()\n",
    "    model = model.cuda()\n",
    "    perturbed_images = []\n",
    "\n",
    "    for i in tqdm(range(0, len(images), batch_size), desc=\"fgsm ensemble\", total=len(images) // batch_size, ncols=100):\n",
    "        batch_images = images[i : i + batch_size]\n",
    "        batch_labels = labels[i : i + batch_size]\n",
    "\n",
    "        x = torch.FloatTensor(batch_images.transpose(0, 3, 1, 2)).cuda()\n",
    "        y = torch.LongTensor(batch_labels).cuda()\n",
    "        x.requires_grad = True\n",
    "        free_mem()\n",
    "\n",
    "        layer_outputs = []\n",
    "        for layer_i in layers_to_use:\n",
    "            outputs = model.predict_from_layer(x, layer_i)\n",
    "            layer_outputs.append(outputs.unsqueeze(1))\n",
    "        ensemble_outputs = torch.cat(layer_outputs, dim=1)\n",
    "\n",
    "        logits = get_cross_max_consensus_logits(ensemble_outputs, k=3)\n",
    "        loss = nn.CrossEntropyLoss()(logits, y)\n",
    "        loss.backward()\n",
    "\n",
    "        perturbed_image = x + epsilon * x.grad.data.sign()\n",
    "        perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "        perturbed_images.append(perturbed_image.detach().cpu().numpy().transpose(0, 2, 3, 1))\n",
    "\n",
    "        x.grad.zero_()\n",
    "    return np.concatenate(perturbed_images, axis=0)\n",
    "\n",
    "\n",
    "def pgd_attack_ensemble(model, images, labels, epsilon, alpha=0.01, num_iter=40, batch_size=128):\n",
    "    def get_cross_max_consensus_logits(outputs: torch.Tensor, k: int) -> torch.Tensor:\n",
    "        Z_hat = outputs - outputs.max(dim=2, keepdim=True)[0]\n",
    "        Z_hat = Z_hat - Z_hat.max(dim=1, keepdim=True)[0]\n",
    "        Y, _ = torch.topk(Z_hat, k, dim=1)\n",
    "        Y = Y[:, -1, :]\n",
    "        return Y\n",
    "\n",
    "    model = model.eval()\n",
    "    model = model.cuda()\n",
    "    perturbed_images = []\n",
    "\n",
    "    for i in tqdm(range(0, len(images), batch_size), desc=\"pgd ensemble\", total=len(images) // batch_size, ncols=100):\n",
    "        batch_images = images[i : i + batch_size]\n",
    "        batch_labels = labels[i : i + batch_size]\n",
    "\n",
    "        x = torch.FloatTensor(batch_images.transpose(0, 3, 1, 2)).cuda()\n",
    "        y = torch.LongTensor(batch_labels).cuda()\n",
    "        \n",
    "        delta = torch.zeros_like(x, requires_grad=True).cuda()\n",
    "        delta.uniform_(-epsilon, epsilon)\n",
    "        delta = torch.clamp(x + delta, 0, 1) - x\n",
    "        \n",
    "        for _ in range(num_iter):\n",
    "            x_adv = x + delta\n",
    "            x_adv.requires_grad = True\n",
    "            free_mem()\n",
    "\n",
    "            layer_outputs = []\n",
    "            for layer_i in layers_to_use:\n",
    "                outputs = model.predict_from_layer(x_adv, layer_i)\n",
    "                layer_outputs.append(outputs.unsqueeze(1))\n",
    "            ensemble_outputs = torch.cat(layer_outputs, dim=1)\n",
    "\n",
    "            logits = get_cross_max_consensus_logits(ensemble_outputs, k=3)\n",
    "            loss = nn.CrossEntropyLoss()(logits, y)\n",
    "            loss.backward()\n",
    "\n",
    "            grad = x_adv.grad.data\n",
    "            delta = delta + alpha * grad.sign()\n",
    "            \n",
    "            delta = torch.clamp(delta, -epsilon, epsilon)\n",
    "            delta = torch.clamp(x + delta, 0, 1) - x\n",
    "            \n",
    "            delta = delta.detach()\n",
    "            delta.requires_grad = True\n",
    "\n",
    "        perturbed_image = torch.clamp(x + delta, 0, 1)\n",
    "        perturbed_images.append(perturbed_image.detach().cpu().numpy().transpose(0, 2, 3, 1))\n",
    "\n",
    "    return np.concatenate(perturbed_images, axis=0)\n",
    "\n",
    "\n",
    "def hcaptcha_mask(images, mask: Image.Image, opacity: int):\n",
    "    # opacity range: 0 (transparent) to 255 (opaque)\n",
    "    def add_overlay(background: Image.Image, overlay: Image.Image, opacity: int) -> Image.Image:\n",
    "        overlay = overlay.resize(background.size)\n",
    "        result = Image.new(\"RGBA\", background.size)\n",
    "        result.paste(background, (0, 0))\n",
    "        mask = Image.new(\"L\", overlay.size, opacity)\n",
    "        result.paste(overlay, (0, 0), mask)\n",
    "        return result\n",
    "\n",
    "    all_perturbed_images = []\n",
    "\n",
    "    to_pil = lambda x: Image.fromarray((x * 255).astype(np.uint8))\n",
    "    to_np = lambda x: np.array(x) / 255.0\n",
    "    for i in tqdm(range(len(images)), desc=\"adding overlay\", ncols=100):\n",
    "        perturbed_image = to_np(add_overlay(to_pil(images[i]), mask, opacity).convert(\"RGB\"))\n",
    "        all_perturbed_images.append(perturbed_image)\n",
    "\n",
    "    return np.array(all_perturbed_images)\n",
    "\n",
    "\n",
    "def eval_layers(backbone_model, images_test_np, labels_test_np, layers_to_use):\n",
    "    layer_acc = []\n",
    "    for layer_i in layers_to_use:\n",
    "        print(f\"evaluating layer={layer_i}\")\n",
    "        linear_model = LinearNet(backbone_model, layer_i).to(\"cuda\")\n",
    "        linear_model.eval()\n",
    "        test_hits, test_count, test_logits = eval_model(linear_model, images_test_np.copy(), labels_test_np.copy())\n",
    "        layer_acc.append(test_hits / test_count)\n",
    "    return {layer_i: acc for layer_i, acc in zip(layers_to_use, layer_acc)}\n",
    "\n",
    "\n",
    "def eval_self_ensemble(backbone_model, images_test, labels_test, layers_to_use, batch_size=128):\n",
    "    def get_cross_max_consensus_logits(outputs: torch.Tensor, k: int) -> torch.Tensor:\n",
    "        Z_hat = outputs - outputs.max(dim=2, keepdim=True)[0]  # subtract the max per-predictor over classes\n",
    "        Z_hat = Z_hat - Z_hat.max(dim=1, keepdim=True)[0]  # subtract the per-class max over predictors\n",
    "        Y, _ = torch.topk(Z_hat, k, dim=1)  # get highest k values per class\n",
    "        Y = Y[:, -1, :]  # get the k-th highest value per class\n",
    "        assert Y.shape == (outputs.shape[0], outputs.shape[2])\n",
    "        assert len(Y.shape) == 2\n",
    "        return Y\n",
    "\n",
    "    backbone_model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(images_test), batch_size), desc=\"ensemble eval\", ncols=100):\n",
    "            batch_images = torch.Tensor(images_test[i : i + batch_size].transpose([0, 3, 1, 2])).cuda()  # transpose to [batch_size, channels, height, width]\n",
    "            batch_labels = labels_test[i : i + batch_size]\n",
    "            layer_outputs = []\n",
    "\n",
    "            for layer_i in layers_to_use:\n",
    "                outputs = backbone_model.predict_from_layer(batch_images, layer_i)\n",
    "                layer_outputs.append(outputs.unsqueeze(1))\n",
    "            ensemble_outputs = torch.cat(layer_outputs, dim=1)  # [batch_size, num_layers, num_classes]\n",
    "\n",
    "            logits = get_cross_max_consensus_logits(ensemble_outputs, k=3)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.append(predictions.cpu().numpy())\n",
    "            all_labels.append(batch_labels)\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    accuracy = np.mean(all_preds == all_labels)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def is_cached(filepath, combination):\n",
    "    if filepath.exists() and filepath.stat().st_size > 0:\n",
    "        lines = filepath.read_text().strip().split(\"\\n\")\n",
    "        lines = [json.loads(line) for line in lines]\n",
    "        for line in lines:\n",
    "            if all(line[k] == v for k, v in combination.items()):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = output_path / \"self_ensemble.jsonl\"\n",
    "fpath.touch(exist_ok=True)\n",
    "\n",
    "combinations = {\n",
    "    \"dataset\": [\"cifar10\", \"cifar100\", \"imagenette\"],\n",
    "    # in future experiments: either set all to True or False, no combinations\n",
    "    \"training_noise\": [False, True],\n",
    "    \"training_shuffle\": [False, True],\n",
    "    \"training_adversarial\": [False, True],\n",
    "}\n",
    "combs = list(product(*combinations.values()))\n",
    "combs = [dict(zip(combinations.keys(), comb)) for comb in combs]\n",
    "comb = combs[0]\n",
    "\n",
    "images_train_np, labels_train_np, images_test_np, labels_test_np, num_classes = get_dataset(comb[\"dataset\"])\n",
    "\n",
    "resolutions = [32, 16, 8, 4]  # arbitrary resolutions to use in stacked images\n",
    "layers_to_use = [20, 30, 35, 40, 45, 50, 52]  # only some layers to save time -> anything below 20 is useless\n",
    "model = get_model(\n",
    "    enable_noise=comb[\"training_noise\"],\n",
    "    enable_random_shuffle=comb[\"training_shuffle\"],\n",
    "    enable_adversarial_training=comb[\"training_adversarial\"],\n",
    "    resolutions=resolutions,\n",
    "    layers_to_use=layers_to_use,\n",
    "    num_classes=num_classes,\n",
    "    images_train_np=images_train_np.copy(),\n",
    "    labels_train_np=labels_train_np.copy(),\n",
    ")\n",
    "model.cuda()\n",
    "model.eval()\n",
    "free_mem()\n",
    "\n",
    "output = {\n",
    "    **comb,\n",
    "    # \"plain_layer_accs\": eval_layers(model, images_test_np.copy(), labels_test_np.copy(), layers_to_use),\n",
    "    # \"plain_ensemble_acc\": eval_self_ensemble(model, images_test_np.copy(), labels_test_np.copy(), layers_to_use),\n",
    "}\n",
    "free_mem()\n",
    "\n",
    "# fgsm_idxs = [20, 30, 35, 40, 45, 50, 52]\n",
    "# for fgsm_idx in fgsm_idxs:\n",
    "#     fgsm_images_test_np = fgsm_attack_layer(model, images_test_np.copy()[:], labels_test_np.copy()[:], epsilon=8 / 255, layer_i=fgsm_idx, batch_size=64)\n",
    "#     output[f\"fgsm_{fgsm_idx}_ensemble_acc\"] = eval_self_ensemble(model, fgsm_images_test_np, labels_test_np.copy(), layers_to_use)\n",
    "#     output[f\"fgsm_{fgsm_idx}_layer_accs\"] = eval_layers(model, fgsm_images_test_np, labels_test_np.copy(), layers_to_use)\n",
    "# free_mem()\n",
    "\n",
    "# fgsmcombined_idxs = [20, 30, 35]\n",
    "# fgsmcombined_images_test_np = fgsm_attack_layer_combined(model, images_test_np.copy()[:], labels_test_np.copy()[:], epsilon=8 / 255, layer_idxs=fgsmcombined_idxs, layer_weights=None, batch_size=64)\n",
    "# output[f\"fgsmcombined_{fgsmcombined_idxs}_ensemble_acc\"] = eval_self_ensemble(model, fgsmcombined_images_test_np, labels_test_np.copy(), layers_to_use)\n",
    "# output[f\"fgsmcombined_{fgsmcombined_idxs}_layer_accs\"] = eval_layers(model, fgsmcombined_images_test_np, labels_test_np.copy(), layers_to_use)\n",
    "# free_mem()\n",
    "\n",
    "# fgsmensemble_images_test_np = fgsm_attack_ensemble(model, images_test_np.copy()[:], labels_test_np.copy()[:], epsilon=8 / 255, batch_size=64)\n",
    "# output[\"fgsmensemble_ensemble_acc\"] = eval_self_ensemble(model, fgsmensemble_images_test_np, labels_test_np.copy(), layers_to_use)\n",
    "# output[\"fgsmensemble_layer_accs\"] = eval_layers(model, fgsmensemble_images_test_np, labels_test_np.copy(), layers_to_use)\n",
    "# free_mem()\n",
    "\n",
    "pgd_idxs = [20, 30, 35, 40, 45, 50, 52]\n",
    "for pgd_idx in pgd_idxs:\n",
    "    pgd_images_test_np = pgd_attack_layer(model, images_test_np.copy()[:], labels_test_np.copy()[:], epsilon=8 / 255, layer_i=pgd_idx, batch_size=64)\n",
    "    output[f\"pgd_{pgd_idx}_ensemble_acc\"] = eval_self_ensemble(model, pgd_images_test_np, labels_test_np.copy(), layers_to_use)\n",
    "    output[f\"pgd_{pgd_idx}_layer_accs\"] = eval_layers(model, pgd_images_test_np, labels_test_np.copy(), layers_to_use)\n",
    "free_mem()\n",
    "\n",
    "pgdcombined_idxs = [20, 30, 35]\n",
    "pgdcombined_images_test_np = pgd_attack_layer_combined(model, images_test_np.copy()[:], labels_test_np.copy()[:], epsilon=8 / 255, layer_idxs=pgdcombined_idxs, layer_weights=None, batch_size=64)\n",
    "output[f\"pgdcombined_{pgdcombined_idxs}_ensemble_acc\"] = eval_self_ensemble(model, pgdcombined_images_test_np, labels_test_np.copy(), layers_to_use)\n",
    "output[f\"pgdcombined_{pgdcombined_idxs}_layer_accs\"] = eval_layers(model, pgdcombined_images_test_np, labels_test_np.copy(), layers_to_use)\n",
    "\n",
    "pgdensemble_images_test_np = pgd_attack_ensemble(model, images_test_np.copy()[:], labels_test_np.copy()[:], epsilon=8 / 255, batch_size=64)\n",
    "output[\"pgdensemble_ensemble_acc\"] = eval_self_ensemble(model, pgdensemble_images_test_np, labels_test_np.copy(), layers_to_use)\n",
    "output[\"pgdensemble_layer_accs\"] = eval_layers(model, pgdensemble_images_test_np, labels_test_np.copy(), layers_to_use)\n",
    "free_mem()\n",
    "\n",
    "# opacities = [0, 1, 2, 4, 8, 16, 32, 64, 128, 255]\n",
    "# for opacity in opacities:\n",
    "#     hcaptcha_images_test_np = hcaptcha_mask(images_test_np.copy(), prerendered_mask, opacity)\n",
    "#     output[f\"mask_{opacity}_layer_accs\"] = eval_layers(model, hcaptcha_images_test_np, labels_test_np.copy(), layers_to_use)\n",
    "#     output[f\"mask_{opacity}_ensemble_acc\"] = eval_self_ensemble(model, hcaptcha_images_test_np, labels_test_np.copy(), layers_to_use)\n",
    "# free_mem()\n",
    "\n",
    "import json\n",
    "print(json.dumps(output, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
