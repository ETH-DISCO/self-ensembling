{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can run locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (2.4.1)\n",
      "Requirement already satisfied: torchvision in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (0.19.1)\n",
      "Requirement already satisfied: numpy in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: tqdm in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (4.66.5)\n",
      "Requirement already satisfied: matplotlib in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: plotnine in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (0.14.0)\n",
      "Requirement already satisfied: pandas in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: filelock in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: mizani~=0.13.0 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from plotnine) (0.13.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from plotnine) (1.14.1)\n",
      "Requirement already satisfied: statsmodels>=0.14.0 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from plotnine) (0.14.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from statsmodels>=0.14.0->plotnine) (0.5.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision numpy tqdm matplotlib plotnine pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "from plotnine import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "\n",
    "\n",
    "device = get_device(disable_mps=False)\n",
    "set_env()\n",
    "\n",
    "\n",
    "#\n",
    "# data\n",
    "#\n",
    "\n",
    "\n",
    "data_path = get_current_dir().parent / \"data\"\n",
    "dataset_path = get_current_dir().parent / \"datasets\"\n",
    "weights_path = get_current_dir().parent / \"weights\"\n",
    "\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "os.makedirs(weights_path, exist_ok=True)\n",
    "\n",
    "\n",
    "dataset = \"cifar100\"\n",
    "\n",
    "if dataset == \"cifar10\":\n",
    "    num_classes = 10\n",
    "    trainset = torchvision.datasets.CIFAR10(root=dataset_path, train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "    testset = torchvision.datasets.CIFAR10(root=dataset_path, train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "    original_images_train_np = np.array(trainset.data)\n",
    "    original_labels_train_np = np.array(trainset.targets)\n",
    "\n",
    "    original_images_test_np = np.array(testset.data)\n",
    "    original_labels_test_np = np.array(testset.targets)\n",
    "\n",
    "elif dataset == \"cifar100\":\n",
    "    num_classes = 100\n",
    "    trainset = torchvision.datasets.CIFAR100(root=dataset_path, train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "    testset = torchvision.datasets.CIFAR100(root=dataset_path, train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "    original_images_train_np = np.array(trainset.data)\n",
    "    original_labels_train_np = np.array(trainset.targets)\n",
    "\n",
    "    original_images_test_np = np.array(testset.data)\n",
    "    original_labels_test_np = np.array(testset.targets)\n",
    "\n",
    "elif dataset == \"imagenette\":\n",
    "    num_classes = 10\n",
    "    imgpath = dataset_path / \"imagenette2\"\n",
    "    transform = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.Resize((224, 224)),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    trainset = torchvision.datasets.Imagenette(root=dataset_path, split=\"train\", download=(not (imgpath / \"train\").exists()), transform=transform)\n",
    "    testset = torchvision.datasets.Imagenette(root=dataset_path, split=\"val\", download=(not (imgpath / \"val\").exists()), transform=transform)\n",
    "\n",
    "    # preallocate\n",
    "    num_train = len(trainset)\n",
    "    num_test = len(testset)\n",
    "    original_images_train_np = np.empty((num_train, 224, 224, 3), dtype=np.uint8)\n",
    "    original_labels_train_np = np.empty(num_train, dtype=np.int64)\n",
    "    original_images_test_np = np.empty((num_test, 224, 224, 3), dtype=np.uint8)\n",
    "    original_labels_test_np = np.empty(num_test, dtype=np.int64)\n",
    "\n",
    "    for idx, (image, label) in tqdm(enumerate(trainset), total=num_train, desc=\"preprocessing trainset\", ncols=100):\n",
    "        original_images_train_np[idx] = (np.transpose(image.numpy(), (1, 2, 0)) * 255).astype(np.uint8)\n",
    "        original_labels_train_np[idx] = label\n",
    "\n",
    "    for idx, (image, label) in tqdm(enumerate(testset), total=num_test, desc=\"preprocessing testset\", ncols=100):\n",
    "        original_images_test_np[idx] = (np.transpose(image.numpy(), (1, 2, 0)) * 255).astype(np.uint8)\n",
    "        original_labels_test_np[idx] = label\n",
    "\n",
    "else:\n",
    "    assert False\n",
    "\n",
    "images_train_np = original_images_train_np / 255.0  # map to [0, 1]\n",
    "images_test_np = original_images_test_np / 255.0\n",
    "labels_train_np = original_labels_train_np\n",
    "labels_test_np = original_labels_test_np\n",
    "\n",
    "\n",
    "#\n",
    "# tuning model\n",
    "#\n",
    "\n",
    "\n",
    "from torchvision.models import resnet152, ResNet152_Weights\n",
    "\n",
    "model = resnet152(weights=ResNet152_Weights.IMAGENET1K_V2)\n",
    "model.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "\n",
    "# \n",
    "# eval\n",
    "# \n",
    "\n",
    "\n",
    "def add_overlay(background: Image.Image, overlay: Image.Image, opacity: int) -> Image.Image:\n",
    "    # opacity range: 0 (transparent) to 255 (opaque)\n",
    "    overlay = overlay.resize(background.size)\n",
    "    result = Image.new(\"RGBA\", background.size)\n",
    "    result.paste(background, (0, 0))\n",
    "    mask = Image.new(\"L\", overlay.size, opacity)\n",
    "    result.paste(overlay, (0, 0), mask)\n",
    "    return result\n",
    "\n",
    "\n",
    "test_mods = []\n",
    "free_mem()\n",
    "\n",
    "images_test_np_copy = images_test_np.copy()\n",
    "labels_test_np_copy = labels_test_np.copy()\n",
    "\n",
    "if True:\n",
    "    args = {\n",
    "        \"opacity\": 12,  # range: 0 (transparent) to 255 (opaque)\n",
    "    }\n",
    "    test_mods.append({\"hcaptcha_mask\": args})\n",
    "    maskpath = get_current_dir().parent / \"data\" / \"masks\" / \"mask.png\"\n",
    "    mask = Image.open(maskpath)\n",
    "    for i in range(len(images_test_np_copy)):\n",
    "        images_test_np_copy[i] = add_overlay(Image.fromarray((255 * images_test_np_copy[i]).astype(np.uint8)), mask, args[\"opacity\"]).convert(\"RGB\")\n",
    "\n",
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
