{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YFmbVrfB2-O"
   },
   "source": [
    "We start from an Imagenet-pretrained ResNet152, replace its first and last layers, make it ready to use multi-resolution inputs generated from a single image, and finetune on CIFAR-100.\n",
    "\n",
    "We then train separate linear layers for each of the intermediate representations on top of a frozen backbone model.\n",
    "\n",
    "From these, we form a self-ensemble. We evalaute its adversarial accuracy on CIFAR-100 using the RobustBench AutoAttack at the end (with the `rand` flag enable).\n",
    "\n",
    "The whole Colab should take ~60 minutes on an A100 GPU and should be self-contained.\n",
    "\n",
    "It should give you above/about SOTA adversarial robustness on CIFAR-100 under $L_\\infty = 8/255$ attacks already, visualize the successfully attacked images and also visualize the class prototypes optimized directly from pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n",
      "Requirement already satisfied: torch in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (2.4.1)\n",
      "Requirement already satisfied: torchvision in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (0.19.1)\n",
      "Requirement already satisfied: numpy in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: tqdm in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (4.66.5)\n",
      "Requirement already satisfied: matplotlib in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: filelock in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/sueszli/dev/self-ensembling/.venv/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!pip install torch torchvision numpy tqdm matplotlib safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiyuMSC4Pwui"
   },
   "outputs": [],
   "source": [
    "from torch.amp import autocast, GradScaler\n",
    "from torchvision.models import resnet152, ResNet152_Weights\n",
    "from types import SimpleNamespace\n",
    "import json\n",
    "from safetensors import save_file\n",
    "import copy\n",
    "import hashlib\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "from torchvision.models import resnet152, ResNet152_Weights\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import datasets, models\n",
    "from torchvision.models import resnet152\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import random\n",
    "import hashlib\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from contextlib import contextmanager\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "#\n",
    "# config\n",
    "#\n",
    "\n",
    "\n",
    "classes_path = Path.cwd().parent / \"data\"\n",
    "dataset_path = Path.cwd().parent / \"datasets\"\n",
    "weights_path = Path.cwd().parent / \"weights\"\n",
    "\n",
    "os.makedirs(classes_path, exist_ok=True)\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "os.makedirs(weights_path, exist_ok=True)\n",
    "\n",
    "\n",
    "#\n",
    "# perf\n",
    "#\n",
    "\n",
    "\n",
    "def free_mem():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "\n",
    "free_mem()\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = (\n",
    "    \"backend:native,\",\n",
    "    \"max_split_size_mb:512,\",\n",
    "    \"garbage_collection_threshold:0.8,\",\n",
    "    \"expandable_segments:True\",\n",
    ")\n",
    "\n",
    "# seed = 41\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def isolated_environment():\n",
    "    # save and restore random states to seperate the random-seed-fixing behavior from attacks\n",
    "    np_random_state = np.random.get_state()\n",
    "    python_random_state = random.getstate()\n",
    "    torch_random_state = torch.get_rng_state()\n",
    "    cuda_random_state = torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None\n",
    "    numpy_print_options = np.get_printoptions()\n",
    "    try:\n",
    "        yield  # execute code block\n",
    "    finally:\n",
    "        np.random.set_state(np_random_state)\n",
    "        random.setstate(python_random_state)\n",
    "        torch.set_rng_state(torch_random_state)\n",
    "        if cuda_random_state:\n",
    "            torch.cuda.set_rng_state_all(cuda_random_state)\n",
    "        np.set_printoptions(**numpy_print_options)\n",
    "\n",
    "\n",
    "#\n",
    "# data\n",
    "#\n",
    "\n",
    "\n",
    "classes = 100\n",
    "\n",
    "if classes == 10:\n",
    "    trainset = datasets.CIFAR10(root=dataset_path, train=True, download=True)\n",
    "    testset = datasets.CIFAR10(root=dataset_path, train=False, download=True)\n",
    "    original_images_train_np = np.array(trainset.data)\n",
    "    original_labels_train_np = np.array(trainset.targets)\n",
    "    original_images_test_np = np.array(testset.data)\n",
    "    original_labels_test_np = np.array(testset.targets)\n",
    "    class_names_cifar10 = json.loads((classes_path / \"cifar10_classes.json\").read_text())\n",
    "elif classes == 100:\n",
    "    trainset = datasets.CIFAR100(root=dataset_path, train=True, download=True)\n",
    "    testset = datasets.CIFAR100(root=dataset_path, train=False, download=True)\n",
    "    original_images_train_np = np.array(trainset.data)\n",
    "    original_labels_train_np = np.array(trainset.targets)\n",
    "    original_images_test_np = np.array(testset.data)\n",
    "    original_labels_test_np = np.array(testset.targets)\n",
    "    class_names_cifar100 = json.loads((classes_path / \"cifar100_classes.json\").read_text())\n",
    "else:\n",
    "    assert False\n",
    "\n",
    "images_train_np = original_images_train_np / 255.0  # scale to [0, 1]\n",
    "images_test_np = original_images_test_np / 255.0\n",
    "labels_train_np = original_labels_train_np\n",
    "labels_test_np = original_labels_test_np\n",
    "\n",
    "\n",
    "#\n",
    "# multi resolution preprocessing (channel layer)\n",
    "#\n",
    "\n",
    "\n",
    "def custom_rand(input_tensor, size):\n",
    "    return torch.Tensor(np.random.rand(*size)).to(\"cuda\")\n",
    "\n",
    "\n",
    "def custom_choices(items, tensor):\n",
    "    return np.random.choice(items, (len(tensor)))\n",
    "\n",
    "\n",
    "resolutions = [32, 16, 8, 4]  # pretty arbitrary\n",
    "shuffle_image_versions_randomly = False  # to shuffle randomly which image is which in the multi-res stack (false in paper)\n",
    "transform = True  # to apply the transformations or not (true in paper)\n",
    "\n",
    "\n",
    "def default_make_multichannel_input(images):\n",
    "    return torch.concatenate([images] * len(resolutions), axis=1)\n",
    "\n",
    "\n",
    "def apply_transformations(images, down_res, up_res, jit_x, jit_y, down_noise, up_noise, contrast, color_amount):\n",
    "    # images = torch.mean(images,axis=1,keepdims=True) # for MNIST alone\n",
    "\n",
    "    images_collected = []\n",
    "    for i in range(images.shape[0]):\n",
    "        image = images[i]\n",
    "        image = torchvision.transforms.functional.adjust_contrast(image, contrast[i])  # changing contrast\n",
    "        image = torch.roll(image, shifts=(jit_x[i], jit_y[i]), dims=(-2, -1))  # shift the result in x and y\n",
    "        image = color_amount[i] * image + torch.mean(image, axis=0, keepdims=True) * (1 - color_amount[i])  # shifting in the color <-> grayscale axis\n",
    "        images_collected.append(image)\n",
    "\n",
    "    images = torch.stack(images_collected, axis=0)\n",
    "\n",
    "    images = F.interpolate(images, size=(down_res, down_res), mode=\"bicubic\")  # descrease the resolution\n",
    "    noise = down_noise * custom_rand(images + 312, (images.shape[0], 3, down_res, down_res)).to(\"cuda\")  # low res noise\n",
    "    images = images + noise\n",
    "\n",
    "    images = F.interpolate(images, size=(up_res, up_res), mode=\"bicubic\")  # increase the resolution\n",
    "    noise = up_noise * custom_rand(images + 812, (images.shape[0], 3, up_res, up_res)).to(\"cuda\")  # high res noise\n",
    "    images = images + noise\n",
    "\n",
    "    images = torch.clip(images, 0, 1)  # clipping to the right range of values\n",
    "    return images\n",
    "\n",
    "\n",
    "def make_multichannel_input(images):\n",
    "    all_channels = []\n",
    "    if transform:\n",
    "        for i, r in enumerate(resolutions):\n",
    "            jit_size = 3  # max size of the x-y jit in each axis, sampled uniformly from -jit_size to +jit_size inclusive\n",
    "            images_now = apply_transformations(\n",
    "                images,\n",
    "                down_res=r,\n",
    "                up_res=32,  # hard coded for CIFAR-10 or CIFAR-100\n",
    "                jit_x=custom_choices(range(-jit_size, jit_size + 1), images + i),  # x-shift\n",
    "                jit_y=custom_choices(range(-jit_size, jit_size + 1), 51 * images + 7 * i + 125 * r),  # y-shift\n",
    "                down_noise=0.2,  # noise standard deviation to be added at the low resolution\n",
    "                up_noise=0.2,  # noise stadard deviation to be added at the high resolution\n",
    "                contrast=custom_choices(np.linspace(0.5, 1.0, 100), 5 + 7 * images + 8 * i + 2 * r),  # change in contrast\n",
    "                color_amount=custom_choices(np.linspace(0.5, 1.0, 100), 5 + 7 * images + 8 * i + 2 * r),  # change in color amount\n",
    "            )\n",
    "            all_channels.append(images_now)\n",
    "    else:\n",
    "        all_channels = [images] * len(resolutions)\n",
    "\n",
    "    if not shuffle_image_versions_randomly:\n",
    "        return torch.concatenate(all_channels, axis=1)\n",
    "    elif shuffle_image_versions_randomly:\n",
    "        indices = torch.randperm(len(all_channels))\n",
    "        shuffled_tensor_list = [all_channels[i] for i in indices]\n",
    "        return torch.concatenate(shuffled_tensor_list, axis=1)\n",
    "\n",
    "\n",
    "demo = False\n",
    "if demo:\n",
    "    sample_images = images_test_np[:5]\n",
    "    for j in [0, 1]:\n",
    "        multichannel_images = make_multichannel_input(torch.Tensor(sample_images.transpose([0, 3, 1, 2])).to(\"cuda\")).detach().cpu().numpy().transpose([0, 2, 3, 1])\n",
    "        N = 1 + multichannel_images.shape[3] // 3\n",
    "        plt.figure(figsize=(N * 5.5, 5))\n",
    "        plt.subplot(1, N, 1)\n",
    "        plt.title(\"original\")\n",
    "        plt.imshow(sample_images[j])\n",
    "        plt.xticks([], [])\n",
    "        plt.yticks([], [])\n",
    "        for i in range(N - 1):\n",
    "            plt.subplot(1, N, i + 2)\n",
    "            plt.title(f\"res={resolutions[i]}\")\n",
    "            plt.imshow(multichannel_images[j, :, :, 3 * i : 3 * (i + 1)])\n",
    "            plt.xticks([], [])\n",
    "            plt.yticks([], [])\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "#\n",
    "# backbone training\n",
    "#\n",
    "\n",
    "\n",
    "def eval_model(model, images_in, labels_in, batch_size=128):\n",
    "    all_preds = []\n",
    "    all_logits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        its = int(np.ceil(float(len(images_in)) / float(batch_size)))\n",
    "\n",
    "        pbar = tqdm(range(its), desc=\"eval\", ncols=100)\n",
    "\n",
    "        for it in pbar:\n",
    "            i1 = it * batch_size\n",
    "            i2 = min([(it + 1) * batch_size, len(images_in)])\n",
    "\n",
    "            inputs = torch.Tensor(images_in[i1:i2].transpose([0, 3, 1, 2])).to(\"cuda\")\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            all_logits.append(outputs.detach().cpu().numpy())\n",
    "\n",
    "            preds = torch.argmax(outputs, axis=-1)\n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_logits = np.concatenate(all_logits, axis=0)\n",
    "\n",
    "    return np.sum(all_preds == labels_in), all_preds.shape[0], all_logits\n",
    "\n",
    "\n",
    "def fgsm_attack(model, xs, ys, epsilon, random_reps=1, batch_size=64):\n",
    "    model = model.eval()\n",
    "\n",
    "    its = int(np.ceil(xs.shape[0] / batch_size))\n",
    "\n",
    "    all_perturbed_images = []\n",
    "\n",
    "    for it in range(its):\n",
    "        i1 = it * batch_size\n",
    "        i2 = min([(it + 1) * batch_size, xs.shape[0]])\n",
    "\n",
    "        x = torch.Tensor(xs[i1:i2].transpose([0, 3, 1, 2])).to(\"cuda\")\n",
    "        y = torch.Tensor(ys[i1:i2]).to(\"cuda\").to(torch.long)\n",
    "\n",
    "        x.requires_grad = True\n",
    "\n",
    "        for _ in range(random_reps):\n",
    "            outputs = model(x)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, y)\n",
    "            loss.backward()\n",
    "\n",
    "        perturbed_image = x + epsilon * x.grad.data.sign()\n",
    "        perturbed_image = torch.clip(perturbed_image, 0, 1)\n",
    "\n",
    "        all_perturbed_images.append(perturbed_image.detach().cpu().numpy().transpose([0, 2, 3, 1]))\n",
    "\n",
    "    return np.concatenate(all_perturbed_images, axis=0)\n",
    "\n",
    "\n",
    "def train_model(model_in, images_in, labels_in, epochs, lr, batch_size, optimizer_in, subset_only, mode, use_adversarial_training, adversarial_epsilon, skip_test_set_eval):\n",
    "    global storing_models\n",
    "    if mode == \"train\":\n",
    "        model_in.train()\n",
    "    elif mode == \"eval\":\n",
    "        model_in.eval()\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "    if subset_only is None:\n",
    "        train_optimizer = optimizer_in(model_in.parameters(), lr=lr)\n",
    "    else:\n",
    "        train_optimizer = optimizer_in(subset_only, lr=lr)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"epoch {epoch+1}/{epochs}\")\n",
    "        if mode == \"train\":\n",
    "            model_in.train()\n",
    "        elif mode == \"eval\":\n",
    "            model_in.eval()\n",
    "\n",
    "        all_hits = []\n",
    "        randomized_ids = np.random.permutation(range(len(images_in)))\n",
    "\n",
    "        its = int(np.ceil(float(len(images_in)) / float(batch_size)))\n",
    "        pbar = tqdm(range(its), desc=\"Training\", ncols=100)\n",
    "        for it in pbar:\n",
    "            i1 = it * batch_size\n",
    "            i2 = min([(it + 1) * batch_size, len(images_in)])\n",
    "\n",
    "            np_images_used = images_in[randomized_ids[i1:i2]]\n",
    "            np_labels_used = labels_in[randomized_ids[i1:i2]]\n",
    "\n",
    "            # very light adversarial training if on\n",
    "            if use_adversarial_training:\n",
    "                attacked_images = fgsm_attack(model_in.eval(), np_images_used[:], np_labels_used[:], epsilon=adversarial_epsilon, random_reps=1, batch_size=batch_size // 2)\n",
    "                np_images_used = attacked_images\n",
    "                np_labels_used = np_labels_used\n",
    "                if mode == \"train\":\n",
    "                    model_in.train()\n",
    "                elif mode == \"eval\":\n",
    "                    model_in.eval()\n",
    "\n",
    "            inputs = torch.Tensor(np_images_used.transpose([0, 3, 1, 2])).to(\"cuda\")\n",
    "            labels = torch.Tensor(np_labels_used).to(\"cuda\").to(torch.long)\n",
    "\n",
    "            # the actual optimization step\n",
    "            train_optimizer.zero_grad()\n",
    "            outputs = model_in(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            train_optimizer.step()\n",
    "\n",
    "            # on the fly eval for the train set batches\n",
    "            preds = torch.argmax(outputs, axis=-1)\n",
    "            acc = torch.mean((preds == labels).to(torch.float), axis=-1)\n",
    "            all_hits.append((preds == labels).to(torch.float).detach().cpu().numpy())\n",
    "            train_accs.append(acc.detach().cpu().numpy())\n",
    "            pbar.set_description(f\"train acc={acc.detach().cpu().numpy()} loss={loss.item()}\")\n",
    "\n",
    "        # end of epoch eval\n",
    "        if not skip_test_set_eval:\n",
    "            with isolated_environment():\n",
    "                eval_model_copy = copy.deepcopy(model_in)\n",
    "                test_hits, test_count, _ = eval_model(eval_model_copy.eval(), images_test_np, labels_test_np)\n",
    "        train_hits = np.sum(np.concatenate(all_hits, axis=0).reshape([-1]))\n",
    "        train_count = np.concatenate(all_hits, axis=0).reshape([-1]).shape[0]\n",
    "        print(f\"e={epoch} train {train_hits} / {train_count} = {train_hits/train_count},  test {test_hits} / {test_count} = {test_hits/test_count}\")\n",
    "        test_accs.append((test_hits / test_count) if ((not skip_test_set_eval) and (test_count > 0)) else 0)\n",
    "\n",
    "    print(\"done\")\n",
    "    return model_in\n",
    "\n",
    "\n",
    "imported_model = resnet152(weights=ResNet152_Weights.IMAGENET1K_V2)\n",
    "\n",
    "in_planes = 3\n",
    "planes = 64\n",
    "stride = 2\n",
    "N = len(resolutions)  # input channels multiplier due to multi-res input\n",
    "conv2 = nn.Conv2d(N * in_planes, planes, kernel_size=7, stride=stride, padding=3, bias=False)\n",
    "imported_model.conv1 = copy.deepcopy(conv2)  # replace pretrained conv with multi-res one\n",
    "\n",
    "\n",
    "class ImportedModelWrapper(nn.Module):\n",
    "    def __init__(self, imported_model, multichannel_fn):\n",
    "        super(ImportedModelWrapper, self).__init__()\n",
    "        self.imported_model = imported_model\n",
    "        self.multichannel_fn = multichannel_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        # custom preprocessing\n",
    "        x = self.multichannel_fn(x)\n",
    "        # resnet default preprocessing\n",
    "        x = F.interpolate(x, size=(224, 224), mode=\"bicubic\")\n",
    "        x = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406] * (x.shape[1] // 3), std=[0.229, 0.224, 0.225] * (x.shape[1] // 3))(x)\n",
    "        x = self.imported_model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "free_mem()\n",
    "wrapped_model = ImportedModelWrapper(imported_model, make_multichannel_input).to(\"cuda\")\n",
    "wrapped_model.multichannel_fn = make_multichannel_input\n",
    "model = copy.deepcopy(wrapped_model)\n",
    "model.multichannel_fn = make_multichannel_input\n",
    "model = model.train()\n",
    "\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "free_mem()\n",
    "# scaler = GradScaler(\"cuda\") # scaler and compiler are not that helpful\n",
    "with torch.autocast(\"cuda\"):\n",
    "    model = train_model(\n",
    "        model_in=model,\n",
    "        images_in=images_train_np,\n",
    "        labels_in=labels_train_np,\n",
    "        epochs=6,  # 2h on A100 gpu\n",
    "        lr=3.3e-5,  # probably not optimal\n",
    "        batch_size=128,  # reduce if you have mem issues\n",
    "        optimizer_in=optim.Adam,\n",
    "        subset_only=None,\n",
    "        mode=\"train\",\n",
    "        use_adversarial_training=False,\n",
    "        adversarial_epsilon=8 / 255,\n",
    "        skip_test_set_eval=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"high\")  # delete from here\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = (  # delete from here\n",
    "    \"backend:native,\",\n",
    "    \"max_split_size_mb:512,\",\n",
    "    \"garbage_collection_threshold:0.8,\",\n",
    "    \"expandable_segments:True\",\n",
    ")\n",
    "\n",
    "\n",
    "def free_mem():  # delete from here\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "\n",
    "#\n",
    "# ensembled training\n",
    "#\n",
    "\n",
    "\n",
    "class WrapModelForResNet152(torch.nn.Module):\n",
    "    def __init__(self, model, multichannel_fn, classes=10):\n",
    "        super(WrapModelForResNet152, self).__init__()\n",
    "        self.multichannel_fn = multichannel_fn\n",
    "        self.model = model\n",
    "        self.classes = classes\n",
    "        self.layer_operations = [\n",
    "            torch.nn.Sequential(\n",
    "                model.conv1,\n",
    "                model.bn1,\n",
    "                model.relu,\n",
    "                model.maxpool,\n",
    "            ),\n",
    "            *model.layer1,\n",
    "            *model.layer2,\n",
    "            *model.layer3,\n",
    "            *model.layer4,\n",
    "            model.avgpool,\n",
    "            model.fc,\n",
    "        ]\n",
    "        self.all_dims = [\n",
    "            3 * 224 * 224 * len(resolutions),\n",
    "            64 * 56 * 56,\n",
    "            *[256 * 56 * 56] * len(model.layer1),\n",
    "            *[512 * 28 * 28] * len(model.layer2),\n",
    "            *[1024 * 14 * 14] * len(model.layer3),\n",
    "            *[2048 * 7 * 7] * len(model.layer4),\n",
    "            2048,\n",
    "            1000,\n",
    "        ]\n",
    "\n",
    "        class BatchNormLinear(nn.Module):\n",
    "            def __init__(self, in_features, out_features, device=\"cuda\"):\n",
    "                super(BatchNormLinear, self).__init__()\n",
    "                self.batch_norm = nn.BatchNorm1d(in_features, device=\"cuda\")  # batch norm\n",
    "                self.linear = nn.Linear(in_features, out_features, device=\"cuda\")  # linear probe\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = self.batch_norm(x)\n",
    "                return self.linear(x)\n",
    "\n",
    "        self.linear_layers = torch.nn.ModuleList([BatchNormLinear(self.all_dims[i], classes, device=\"cuda\") for i in range(len(self.all_dims))])\n",
    "\n",
    "    def prepare_input(self, x):  # preprocessing\n",
    "        # custom preprocessing\n",
    "        x = self.multichannel_fn(x)\n",
    "        # resnet default preprocessing\n",
    "        x = F.interpolate(x, size=(224, 224), mode=\"bicubic\")\n",
    "        x = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406] * (x.shape[1] // 3), std=[0.229, 0.224, 0.225] * (x.shape[1] // 3))(x)\n",
    "        return x\n",
    "\n",
    "    def forward_until(self, x, layer_id):  # forward until layer\n",
    "        x = self.prepare_input(x)\n",
    "\n",
    "        for l in range(layer_id):\n",
    "            if list(x.shape)[1:] == [2048, 1, 1]:\n",
    "                x = x.reshape([-1, 2048])\n",
    "\n",
    "            x = self.layer_operations[l](x)\n",
    "        return x\n",
    "\n",
    "    def forward_after(self, x, layer_id):  # forward after layer\n",
    "        x = self.prepare_input(x)\n",
    "        for l in range(layer_id, len(self.layer_operations)):\n",
    "            if list(x.shape)[1:] == [2048, 1, 1]:\n",
    "                x = x.reshape([-1, 2048])\n",
    "\n",
    "            x = self.layer_operations[l](x)\n",
    "        return x\n",
    "\n",
    "    def predict_from_layer(self, x, l):  # predict in layer\n",
    "        x = self.forward_until(x, l)\n",
    "        x = x.reshape([x.shape[0], -1])\n",
    "        return self.linear_layers[l](x)\n",
    "\n",
    "    def predict_from_several_layers(self, x, layers):  # predict in several layers\n",
    "        x = self.prepare_input(x)\n",
    "        outputs = dict()\n",
    "        outputs[0] = self.linear_layers[0](x.reshape([x.shape[0], -1]))\n",
    "        for l in range(len(self.layer_operations)):\n",
    "            if list(x.shape)[1:] == [2048, 1, 1]:\n",
    "                x = x.reshape([-1, 2048])\n",
    "\n",
    "            x = self.layer_operations[l](x)\n",
    "\n",
    "            if l in layers:\n",
    "                outputs[l + 1] = self.linear_layers[l + 1](x.reshape([x.shape[0], -1]))\n",
    "        return outputs\n",
    "\n",
    "\n",
    "free_mem()\n",
    "resnet152_wrapper = WrapModelForResNet152(model.imported_model, make_multichannel_input, classes=classes)\n",
    "resnet152_wrapper.multichannel_fn = make_multichannel_input\n",
    "resnet152_wrapper = resnet152_wrapper.to(\"cuda\")\n",
    "assert all([resnet152_wrapper.predict_from_layer(torch.Tensor(np.zeros((2, 3, 32, 32))).cuda(), layer_i).shape == torch.Size([2, classes]) for layer_i in range(53)])  # shape is [2, num_classes]\n",
    "\n",
    "\n",
    "class LinearNet(nn.Module):  # just for a single intermediate layer\n",
    "    def __init__(self, model, layer_i):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.model = model\n",
    "        self.layer_i = layer_i\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.model.predict_from_layer(inputs, self.layer_i)\n",
    "\n",
    "\n",
    "free_mem()\n",
    "backbone_model = resnet152_wrapper.cpu()\n",
    "backbone_model = copy.deepcopy(backbone_model)\n",
    "backbone_model = backbone_model.to(\"cuda\")\n",
    "backbone_model.eval()\n",
    "linear_model = LinearNet(backbone_model, 5).to(\"cuda\")  # just to have it ready\n",
    "\n",
    "linear_layers_collected_dict = dict()\n",
    "layers_to_use = [20, 30, 35, 40, 45, 50, 52]  # just a subset - super early ones are bad on anything harder than CIFAR-10\n",
    "for layer_i in reversed(layers_to_use):\n",
    "    print(f\"------- training layer={layer_i}\")\n",
    "\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "\n",
    "    free_mem()\n",
    "    linear_model.layer_i = layer_i\n",
    "    linear_model.fixed_mode = \"train\"\n",
    "    with torch.autocast(\"cuda\"):\n",
    "        linear_model = train_model(\n",
    "            model_in=linear_model,\n",
    "            images_in=images_train_np[:],\n",
    "            labels_in=labels_train_np[:],\n",
    "            epochs=1,\n",
    "            lr=3.3e-5,  # not optimal\n",
    "            batch_size=32,  # reduce on mem issues\n",
    "            optimizer_in=optim.Adam,\n",
    "            subset_only=linear_model.model.linear_layers[layer_i].parameters(),  # just the linear projection\n",
    "            mode=\"train\",\n",
    "            use_adversarial_training=False,\n",
    "            adversarial_epsilon=None,\n",
    "            skip_test_set_eval=False,\n",
    "        )\n",
    "    linear_layers_collected_dict[layer_i] = copy.deepcopy(backbone_model.linear_layers[layer_i])\n",
    "\n",
    "# load everything to a single model\n",
    "for layer_i in layers_to_use:\n",
    "    print(f\"training layer {layer_i} in {layers_to_use}\")\n",
    "    backbone_model.linear_layers[layer_i] = copy.deepcopy(linear_layers_collected_dict[layer_i])\n",
    "backbone_model.cuda()\n",
    "backbone_model.eval()\n",
    "\n",
    "# save the model\n",
    "save_file(backbone_model, weights_path / \"original_model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# benchmarking\n",
    "#\n",
    "\n",
    "test_acc_by_layer = []\n",
    "for layer_i in layers_to_use:\n",
    "    linear_model = LinearNet(backbone_model, layer_i).to(\"cuda\")\n",
    "    linear_model.eval()\n",
    "    test_hits, test_count, _ = eval_model(linear_model, images_test_np, labels_test_np)\n",
    "    test_acc_by_layer.append(test_hits / test_count)\n",
    "    print(f\"layer={layer_i} test={test_hits}/{test_count}={test_hits/test_count}\")\n",
    "\n",
    "plt.figure(figsize=(7, 5), dpi=100)\n",
    "plt.title(\"Accuracy at intermediate layers\", fontsize=14)\n",
    "plt.plot(layers_to_use, test_acc_by_layer, marker=\"o\", color=\"navy\", label=\"Test\")\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel(\"Layer\", fontsize=14)\n",
    "plt.ylabel(\"Accuracy\", fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOy7xTZTcy0op4Ph2y/HCG0",
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
