{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting git+https://github.com/RobustBench/robustbench.git\n",
      "  Cloning https://github.com/RobustBench/robustbench.git to /scratch/yjabary/apptainer_env/venv/.local/pip-req-build-4r9ui924\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/RobustBench/robustbench.git /scratch/yjabary/apptainer_env/venv/.local/pip-req-build-4r9ui924\n",
      "  Resolved https://github.com/RobustBench/robustbench.git to commit 776bc95bb4167827fb102a32ac5aea62e46cfaab\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (0.20.1)\n",
      "Requirement already satisfied: numpy in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: tqdm in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (4.66.6)\n",
      "Requirement already satisfied: matplotlib in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (3.9.2)\n",
      "Requirement already satisfied: pandas in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: filelock in /scratch/yjabary/apptainer_env/site_packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Collecting autoattack@ git+https://github.com/fra31/auto-attack.git@a39220048b3c9f2cca9a4d3a54604793c68eca7e#egg=autoattack (from robustbench==1.1)\n",
      "  Cloning https://github.com/fra31/auto-attack.git (to revision a39220048b3c9f2cca9a4d3a54604793c68eca7e) to /scratch/yjabary/apptainer_env/venv/.local/pip-install-f9xp7vxk/autoattack_cbc2b2df13984972ac1697e68aea04a0\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/fra31/auto-attack.git /scratch/yjabary/apptainer_env/venv/.local/pip-install-f9xp7vxk/autoattack_cbc2b2df13984972ac1697e68aea04a0\n",
      "  Running command git rev-parse -q --verify 'sha^a39220048b3c9f2cca9a4d3a54604793c68eca7e'\n",
      "  Running command git fetch -q https://github.com/fra31/auto-attack.git a39220048b3c9f2cca9a4d3a54604793c68eca7e\n",
      "  Resolved https://github.com/fra31/auto-attack.git to commit a39220048b3c9f2cca9a4d3a54604793c68eca7e\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torchdiffeq (from robustbench==1.1)\n",
      "  Downloading torchdiffeq-0.2.4-py3-none-any.whl.metadata (440 bytes)\n",
      "Collecting geotorch (from robustbench==1.1)\n",
      "  Downloading geotorch-0.3.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: requests>=2.25.0 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from robustbench==1.1) (2.32.3)\n",
      "Collecting timm>=0.9.0 (from robustbench==1.1)\n",
      "  Downloading timm-1.0.11-py3-none-any.whl.metadata (48 kB)\n",
      "Collecting gdown==5.1.0 (from robustbench==1.1)\n",
      "  Downloading gdown-5.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: pyyaml in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from robustbench==1.1) (6.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from gdown==5.1.0->robustbench==1.1) (4.12.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from requests>=2.25.0->robustbench==1.1) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from requests>=2.25.0->robustbench==1.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from requests>=2.25.0->robustbench==1.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from requests>=2.25.0->robustbench==1.1) (2024.8.30)\n",
      "Collecting huggingface_hub (from timm>=0.9.0->robustbench==1.1)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors (from timm>=0.9.0->robustbench==1.1)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting scipy>=1.4.0 (from torchdiffeq->robustbench==1.1)\n",
      "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /scratch/yjabary/apptainer_env/venv/lib/python3.10/site-packages (from beautifulsoup4->gdown==5.1.0->robustbench==1.1) (2.6)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown==5.1.0->robustbench==1.1)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading gdown-5.1.0-py3-none-any.whl (17 kB)\n",
      "Downloading timm-1.0.11-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading geotorch-0.3.0-py3-none-any.whl (54 kB)\n",
      "Downloading torchdiffeq-0.2.4-py3-none-any.whl (32 kB)\n",
      "Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m263.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Building wheels for collected packages: robustbench, autoattack\n",
      "  Building wheel for robustbench (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for robustbench: filename=robustbench-1.1-py3-none-any.whl size=164229 sha256=1adda3316e58f0877b23a3e8ad29ca3f7535b9ad57e1aa33c2ea88f07b82c48f\n",
      "  Stored in directory: /scratch/yjabary/apptainer_env/venv/.local/pip-ephem-wheel-cache-5qtwq1uz/wheels/a6/95/83/6461b55d0c3f761c07216e248dfa511f4d780da4c1e00294b8\n",
      "  Building wheel for autoattack (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for autoattack: filename=autoattack-0.1-py3-none-any.whl size=36249 sha256=0a593a791506479ee5363ff676f9b4d868aa4f079c97aae7be013eea5b749136\n",
      "  Stored in directory: /scratch/yjabary/apptainer_env/venv/.local/pip-ephem-wheel-cache-5qtwq1uz/wheels/72/59/98/10a8eb862dabff57be1d1e3b1a3664acc78da7667d2c835584\n",
      "Successfully built robustbench autoattack\n",
      "Installing collected packages: autoattack, scipy, safetensors, PySocks, huggingface_hub, gdown, torchdiffeq, geotorch, timm, robustbench\n",
      "Successfully installed PySocks-1.7.1 autoattack-0.1 gdown-5.1.0 geotorch-0.3.0 huggingface_hub-0.26.2 robustbench-1.1 safetensors-0.4.5 scipy-1.14.1 timm-1.0.11 torchdiffeq-0.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision numpy tqdm matplotlib plotnine pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet152-f82ba261.pth\" to /scratch/yjabary/apptainer_env/torch_cache/hub/checkpoints/resnet152-f82ba261.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 230M/230M [00:00<00:00, 315MB/s]\n",
      "train acc=0.1875 loss=3.655247449874878: 100%|████████████████████| 391/391 [02:51<00:00,  2.28it/s]\n",
      "Eval: 100%|█████████████████████████████████████████████████████████| 79/79 [00:17<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=0 train 3100.0 / 50000 = 0.06199999898672104,  test 1408 / 10000 = 0.1408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train acc=0.2750000059604645 loss=2.799875259399414: 100%|████████| 391/391 [02:51<00:00,  2.27it/s]\n",
      "Eval: 100%|█████████████████████████████████████████████████████████| 79/79 [00:17<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=1 train 11325.0 / 50000 = 0.226500004529953,  test 3157 / 10000 = 0.3157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train acc=0.4000000059604645 loss=2.1474509239196777: 100%|███████| 391/391 [02:51<00:00,  2.29it/s]\n",
      "Eval: 100%|█████████████████████████████████████████████████████████| 79/79 [00:20<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=2 train 19513.0 / 50000 = 0.3902600109577179,  test 4467 / 10000 = 0.4467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train acc=0.612500011920929 loss=1.7065765857696533: 100%|████████| 391/391 [02:51<00:00,  2.28it/s]\n",
      "Eval: 100%|█████████████████████████████████████████████████████████| 79/79 [00:18<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=3 train 24799.0 / 50000 = 0.49597999453544617,  test 5103 / 10000 = 0.5103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train acc=0.5875000357627869 loss=1.5127522945404053: 100%|███████| 391/391 [02:51<00:00,  2.28it/s]\n",
      "Eval: 100%|█████████████████████████████████████████████████████████| 79/79 [00:19<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=4 train 28234.0 / 50000 = 0.5646799802780151,  test 5613 / 10000 = 0.5613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train acc=0.675000011920929 loss=1.0542172193527222: 100%|████████| 391/391 [02:52<00:00,  2.27it/s]\n",
      "Eval: 100%|█████████████████████████████████████████████████████████| 79/79 [00:16<00:00,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=5 train 30811.0 / 50000 = 0.6162199974060059,  test 5889 / 10000 = 0.5889\n",
      "\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "from plotnine import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "assert gpu_memory >= 79, \"at least 80GB gpu memory required\"\n",
    "\n",
    "set_env()\n",
    "\n",
    "\n",
    "#\n",
    "# data\n",
    "#\n",
    "\n",
    "\n",
    "data_path = get_current_dir().parent / \"data\"\n",
    "dataset_path = get_current_dir().parent / \"datasets\"\n",
    "weights_path = get_current_dir().parent / \"weights\"\n",
    "\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "os.makedirs(weights_path, exist_ok=True)\n",
    "\n",
    "\n",
    "dataset = \"imagenette\"\n",
    "\n",
    "if dataset == \"cifar10\":\n",
    "    num_classes = 10\n",
    "    trainset = torchvision.datasets.CIFAR10(root=dataset_path, train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "    testset = torchvision.datasets.CIFAR10(root=dataset_path, train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "    original_images_train_np = np.array(trainset.data)\n",
    "    original_labels_train_np = np.array(trainset.targets)\n",
    "\n",
    "    original_images_test_np = np.array(testset.data)\n",
    "    original_labels_test_np = np.array(testset.targets)\n",
    "\n",
    "elif dataset == \"cifar100\":\n",
    "    num_classes = 100\n",
    "    trainset = torchvision.datasets.CIFAR100(root=dataset_path, train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "    testset = torchvision.datasets.CIFAR100(root=dataset_path, train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "    original_images_train_np = np.array(trainset.data)\n",
    "    original_labels_train_np = np.array(trainset.targets)\n",
    "\n",
    "    original_images_test_np = np.array(testset.data)\n",
    "    original_labels_test_np = np.array(testset.targets)\n",
    "\n",
    "elif dataset == \"imagenette\":\n",
    "    num_classes = 10\n",
    "    imgpath = dataset_path / \"imagenette2\"\n",
    "    transform = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.Resize((224, 224)),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    trainset = torchvision.datasets.Imagenette(root=dataset_path, split=\"train\", download=(not (imgpath / \"train\").exists()), transform=transform)\n",
    "    testset = torchvision.datasets.Imagenette(root=dataset_path, split=\"val\", download=(not (imgpath / \"val\").exists()), transform=transform)\n",
    "\n",
    "    # preallocate\n",
    "    num_train = len(trainset)\n",
    "    num_test = len(testset)\n",
    "    original_images_train_np = np.empty((num_train, 224, 224, 3), dtype=np.uint8)\n",
    "    original_labels_train_np = np.empty(num_train, dtype=np.int64)\n",
    "    original_images_test_np = np.empty((num_test, 224, 224, 3), dtype=np.uint8)\n",
    "    original_labels_test_np = np.empty(num_test, dtype=np.int64)\n",
    "\n",
    "    for idx, (image, label) in tqdm(enumerate(trainset), total=num_train, desc=\"preprocessing trainset\", ncols=100):\n",
    "        original_images_train_np[idx] = (np.transpose(image.numpy(), (1, 2, 0)) * 255).astype(np.uint8)\n",
    "        original_labels_train_np[idx] = label\n",
    "\n",
    "    for idx, (image, label) in tqdm(enumerate(testset), total=num_test, desc=\"preprocessing testset\", ncols=100):\n",
    "        original_images_test_np[idx] = (np.transpose(image.numpy(), (1, 2, 0)) * 255).astype(np.uint8)\n",
    "        original_labels_test_np[idx] = label\n",
    "\n",
    "else:\n",
    "    assert False\n",
    "\n",
    "images_train_np = original_images_train_np / 255.0  # map to [0, 1]\n",
    "images_test_np = original_images_test_np / 255.0\n",
    "labels_train_np = original_labels_train_np\n",
    "labels_test_np = original_labels_test_np\n",
    "\n",
    "\n",
    "#\n",
    "# preprocessing\n",
    "#\n",
    "\n",
    "\n",
    "ENABLE_NOISE = False  # just stack, don't do anything else\n",
    "ENABLE_RANDOM_SHUFFLE = False  # shuffle the images randomly in the multi-res stack\n",
    "\n",
    "resolutions = [32, 16, 8, 4]  # pretty arbitrary\n",
    "down_noise = 0.2  # noise standard deviation to be added at the low resolution\n",
    "up_noise = 0.2  # noise stadard deviation to be added at the high resolution\n",
    "jit_size = 3  # max size of the x-y jit in each axis, sampled uniformly from -jit_size to +jit_size inclusive\n",
    "up_res = 32  # hard coded for CIFAR-10 or CIFAR-100\n",
    "\n",
    "\n",
    "def make_multichannel_input(images):\n",
    "    if not ENABLE_NOISE:\n",
    "        return torch.concatenate([images] * len(resolutions), axis=1)  # don't do anything\n",
    "\n",
    "    all_channels = []\n",
    "    for i, r in enumerate(resolutions):\n",
    "\n",
    "        def apply_transformations(images, down_res, up_res, jit_x, jit_y, down_noise, up_noise, contrast, color_amount):\n",
    "            # images = torch.mean(images, axis=1, keepdims=True) # only for mnist\n",
    "            images_collected = []\n",
    "            for i in range(images.shape[0]):\n",
    "                image = images[i]\n",
    "                image = torchvision.transforms.functional.adjust_contrast(image, contrast[i])  # changing contrast\n",
    "                image = torch.roll(image, shifts=(jit_x[i], jit_y[i]), dims=(-2, -1))  # shift the result in x and y\n",
    "                image = color_amount[i] * image + torch.mean(image, axis=0, keepdims=True) * (1 - color_amount[i])  # shifting in the color <-> grayscale axis\n",
    "                images_collected.append(image)\n",
    "            images = torch.stack(images_collected, axis=0)\n",
    "            images = F.interpolate(images, size=(down_res, down_res), mode=\"bicubic\")  # descrease the resolution\n",
    "            noise = down_noise * torch.Tensor(np.random.rand(images.shape[0], 3, down_res, down_res)).to(\"cuda\")  # low res noise\n",
    "            images = images + noise\n",
    "            images = F.interpolate(images, size=(up_res, up_res), mode=\"bicubic\")  # increase the resolution\n",
    "            noise = up_noise * torch.Tensor(np.random.rand(images.shape[0], 3, up_res, up_res)).to(\"cuda\")  # high res noise\n",
    "            images = images + noise\n",
    "            images = torch.clip(images, 0, 1)  # clipping to the right range of values\n",
    "            return images\n",
    "\n",
    "        images_now = apply_transformations(\n",
    "            images,\n",
    "            down_res=r,\n",
    "            up_res=up_res,\n",
    "            jit_x=np.random.choice(range(-jit_size, jit_size + 1), len(images + i)),  # x-shift,\n",
    "            jit_y=np.random.choice(range(-jit_size, jit_size + 1), len(51 * images + 7 * i + 125 * r)),  # y-shift\n",
    "            down_noise=down_noise,\n",
    "            up_noise=up_noise,\n",
    "            contrast=np.random.choice(np.linspace(0.7, 1.5, 100), len(7 + 3 * images + 9 * i + 5 * r)),  # change in contrast,\n",
    "            color_amount=np.random.choice(np.linspace(0.5, 1.0, 100), len(5 + 7 * images + 8 * i + 2 * r)),  # change in color amount\n",
    "        )\n",
    "        all_channels.append(images_now)\n",
    "\n",
    "    if not ENABLE_RANDOM_SHUFFLE:\n",
    "        return torch.concatenate(all_channels, axis=1)\n",
    "    elif ENABLE_RANDOM_SHUFFLE:\n",
    "        indices = torch.randperm(len(all_channels))\n",
    "        shuffled_tensor_list = [all_channels[i] for i in indices]\n",
    "        return torch.concatenate(shuffled_tensor_list, axis=1)\n",
    "\n",
    "\n",
    "plot = False\n",
    "if plot:\n",
    "    sample_images = images_test_np[:5]\n",
    "    for j in [0, 1]:\n",
    "        multichannel_images = make_multichannel_input(torch.Tensor(sample_images.transpose([0, 3, 1, 2])).to(\"cuda\")).detach().cpu().numpy().transpose([0, 2, 3, 1])\n",
    "        N = 1 + multichannel_images.shape[3] // 3\n",
    "        plt.figure(figsize=(N * 5.5, 5))\n",
    "        plt.subplot(1, N, 1)\n",
    "        plt.title(\"original\")\n",
    "        plt.imshow(sample_images[j])\n",
    "        plt.xticks([], [])\n",
    "        plt.yticks([], [])\n",
    "        for i in range(N - 1):\n",
    "            plt.subplot(1, N, i + 2)\n",
    "            plt.title(f\"res={resolutions[i]}\")\n",
    "            plt.imshow(multichannel_images[j, :, :, 3 * i : 3 * (i + 1)])\n",
    "            plt.xticks([], [])\n",
    "            plt.yticks([], [])\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "#\n",
    "# train and eval loop\n",
    "#\n",
    "\n",
    "\n",
    "@timeit\n",
    "def eval_model(model, images_in, labels_in, batch_size=128):\n",
    "    all_preds = []\n",
    "    all_logits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        its = int(np.ceil(float(len(images_in)) / float(batch_size)))\n",
    "        pbar = tqdm(range(its), desc=\"eval\", ncols=100)\n",
    "        for it in pbar:\n",
    "            i1 = it * batch_size\n",
    "            i2 = min([(it + 1) * batch_size, len(images_in)])\n",
    "\n",
    "            inputs = torch.Tensor(images_in[i1:i2].transpose([0, 3, 1, 2])).to(\"cuda\")\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            all_logits.append(outputs.detach().cpu().numpy())\n",
    "            preds = torch.argmax(outputs, axis=-1)\n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_logits = np.concatenate(all_logits, axis=0)\n",
    "    return np.sum(all_preds == labels_in), all_preds.shape[0], all_logits\n",
    "\n",
    "\n",
    "def fgsm_attack(model, xs, ys, epsilon, random_reps, batch_size):\n",
    "    model = model.eval()\n",
    "    model = model.cuda()\n",
    "\n",
    "    all_perturbed_images = []\n",
    "\n",
    "    its = int(np.ceil(xs.shape[0] / batch_size))\n",
    "    for it in range(its):\n",
    "        i1 = it * batch_size\n",
    "        i2 = min([(it + 1) * batch_size, xs.shape[0]])\n",
    "\n",
    "        x = torch.Tensor(xs[i1:i2].transpose([0, 3, 1, 2])).to(\"cuda\")\n",
    "        y = torch.Tensor(ys[i1:i2]).to(\"cuda\").to(torch.long)\n",
    "        x.requires_grad = True\n",
    "\n",
    "        for _ in range(random_reps):\n",
    "            outputs = model(x)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, y)\n",
    "            loss.backward()\n",
    "\n",
    "        perturbed_image = x + epsilon * x.grad.data.sign()\n",
    "        perturbed_image = torch.clip(perturbed_image, 0, 1)\n",
    "        all_perturbed_images.append(perturbed_image.detach().cpu().numpy().transpose([0, 2, 3, 1]))\n",
    "    return np.concatenate(all_perturbed_images, axis=0)\n",
    "\n",
    "\n",
    "@timeit\n",
    "def train_model(model_in, images_in, labels_in, epochs, lr, batch_size, optimizer_in, subset_only, mode, use_adversarial_training, adversarial_epsilon, skip_test_set_eval):\n",
    "    global storing_models\n",
    "\n",
    "    if mode == \"train\":\n",
    "        model_in.train()\n",
    "    elif mode == \"eval\":\n",
    "        model_in.eval()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if subset_only is None:\n",
    "        train_optimizer = optimizer_in(model_in.parameters(), lr=lr)\n",
    "    else:\n",
    "        train_optimizer = optimizer_in(subset_only, lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"epoch={epoch}/{epochs}\")\n",
    "        if mode == \"train\":  # avoid random flips\n",
    "            model_in.train()\n",
    "        elif mode == \"eval\":\n",
    "            model_in.eval()\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        all_hits = []\n",
    "\n",
    "        randomized_ids = np.random.permutation(range(len(images_in)))\n",
    "        its = int(np.ceil(float(len(images_in)) / float(batch_size)))\n",
    "        pbar = tqdm(range(its), desc=\"Training\", ncols=100)\n",
    "        for it in pbar:\n",
    "            i1 = it * batch_size\n",
    "            i2 = min([(it + 1) * batch_size, len(images_in)])\n",
    "\n",
    "            np_images_used = images_in[randomized_ids[i1:i2]]\n",
    "            np_labels_used = labels_in[randomized_ids[i1:i2]]\n",
    "\n",
    "            if use_adversarial_training:  # very light adversarial training if on\n",
    "                attacked_images = fgsm_attack(model_in.eval(), np_images_used[:], np_labels_used[:], epsilon=adversarial_epsilon, random_reps=1, batch_size=batch_size // 2)\n",
    "                np_images_used = attacked_images\n",
    "                np_labels_used = np_labels_used\n",
    "                if mode == \"train\":\n",
    "                    model_in.train()\n",
    "                elif mode == \"eval\":\n",
    "                    model_in.eval()\n",
    "\n",
    "            inputs = torch.Tensor(np_images_used.transpose([0, 3, 1, 2])).to(\"cuda\")\n",
    "            labels = torch.Tensor(np_labels_used).to(\"cuda\").to(torch.long)\n",
    "\n",
    "            # the actual optimization step\n",
    "            train_optimizer.zero_grad()\n",
    "            outputs = model_in(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            train_optimizer.step()\n",
    "\n",
    "            # on the fly eval for the train set batches\n",
    "            preds = torch.argmax(outputs, axis=-1)\n",
    "            acc = torch.mean((preds == labels).to(torch.float), axis=-1)\n",
    "            all_hits.append((preds == labels).to(torch.float).detach().cpu().numpy())\n",
    "            train_accs.append(acc.detach().cpu().numpy())\n",
    "\n",
    "            pbar.set_description(f\"train acc={acc.detach().cpu().numpy()} loss={loss.item()}\")\n",
    "\n",
    "        if not skip_test_set_eval:\n",
    "            with isolated_environment():\n",
    "                eval_model_copy = copy.deepcopy(model_in)\n",
    "                test_hits, test_count, _ = eval_model(eval_model_copy.eval(), images_test_np, labels_test_np)\n",
    "\n",
    "        # end of epoch eval\n",
    "        train_hits = np.sum(np.concatenate(all_hits, axis=0).reshape([-1]))\n",
    "        train_count = np.concatenate(all_hits, axis=0).reshape([-1]).shape[0]\n",
    "        print(f\"e={epoch} train {train_hits} / {train_count} = {train_hits/train_count},  test {test_hits} / {test_count} = {test_hits/test_count}\")\n",
    "\n",
    "        test_accs.append((test_hits / test_count) if (test_count > 0) else 0)\n",
    "    return model_in\n",
    "\n",
    "\n",
    "#\n",
    "# training backbone\n",
    "#\n",
    "\n",
    "\n",
    "from torchvision.models import resnet152, ResNet152_Weights\n",
    "\n",
    "imported_model = resnet152(weights=ResNet152_Weights.IMAGENET1K_V2)\n",
    "\n",
    "# replace first conv layer with multi-res one\n",
    "in_planes = 3\n",
    "planes = 64\n",
    "stride = 2\n",
    "N = len(resolutions)  # input channels multiplier due to multi-res input\n",
    "conv2 = nn.Conv2d(in_channels=N * in_planes, out_channels=planes, kernel_size=7, stride=stride, padding=3, bias=False)\n",
    "imported_model.conv1 = copy.deepcopy(conv2)\n",
    "\n",
    "# set num of classes in final layer\n",
    "imported_model.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "\n",
    "class ImportedModelWrapper(nn.Module):\n",
    "    def __init__(self, imported_model, multichannel_fn):\n",
    "        super(ImportedModelWrapper, self).__init__()\n",
    "        self.imported_model = imported_model\n",
    "        self.multichannel_fn = multichannel_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        # multichannel input\n",
    "        x = self.multichannel_fn(x)\n",
    "        # imagenet preprocessing\n",
    "        x = F.interpolate(x, size=(224, 224), mode=\"bicubic\")\n",
    "        x = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406] * (x.shape[1] // 3), std=[0.229, 0.224, 0.225] * (x.shape[1] // 3))(x)\n",
    "        x = self.imported_model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "wrapped_model = ImportedModelWrapper(imported_model, make_multichannel_input).to(\"cuda\")\n",
    "wrapped_model.multichannel_fn = make_multichannel_input\n",
    "model = copy.deepcopy(wrapped_model)\n",
    "model.multichannel_fn = make_multichannel_input\n",
    "model.train()\n",
    "\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "ENABLE_ADVERSARIAL_TRAINING = False\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# with torch.autocast(\"cuda\"):\n",
    "model = train_model(\n",
    "    model_in=model,\n",
    "    images_in=images_train_np,\n",
    "    labels_in=labels_train_np,\n",
    "    epochs=6,\n",
    "    lr=3.3e-5,  # simple grid search, likely not optimal\n",
    "    batch_size=128,\n",
    "    optimizer_in=optim.Adam,\n",
    "    subset_only=None,\n",
    "    mode=\"train\",\n",
    "    use_adversarial_training=ENABLE_ADVERSARIAL_TRAINING,\n",
    "    adversarial_epsilon=8 / 255,\n",
    "    skip_test_set_eval=False,\n",
    ")\n",
    "\n",
    "\n",
    "#\n",
    "# training self ensemble layers\n",
    "#\n",
    "\n",
    "\n",
    "class WrapModelForResNet152(torch.nn.Module):\n",
    "    def __init__(self, model, multichannel_fn, num_classes=10):\n",
    "        super(WrapModelForResNet152, self).__init__()\n",
    "        self.multichannel_fn = multichannel_fn\n",
    "        self.model = model\n",
    "        self.num_classes = num_classes\n",
    "        self.layer_operations = [\n",
    "            torch.nn.Sequential(\n",
    "                model.conv1,\n",
    "                model.bn1,\n",
    "                model.relu,\n",
    "                model.maxpool,\n",
    "            ),\n",
    "            *model.layer1,\n",
    "            *model.layer2,\n",
    "            *model.layer3,\n",
    "            *model.layer4,\n",
    "            model.avgpool,\n",
    "            model.fc,\n",
    "        ]\n",
    "        self.all_dims = [\n",
    "            3 * 224 * 224 * len(resolutions),\n",
    "            64 * 56 * 56,\n",
    "            *[256 * 56 * 56] * len(model.layer1),\n",
    "            *[512 * 28 * 28] * len(model.layer2),\n",
    "            *[1024 * 14 * 14] * len(model.layer3),\n",
    "            *[2048 * 7 * 7] * len(model.layer4),\n",
    "            2048,\n",
    "            1000,\n",
    "        ]\n",
    "\n",
    "        class BatchNormLinear(nn.Module):\n",
    "            def __init__(self, in_features, out_features, device=\"cuda\"):\n",
    "                super(BatchNormLinear, self).__init__()\n",
    "                self.batch_norm = nn.BatchNorm1d(in_features, device=device)\n",
    "                self.linear = nn.Linear(in_features, out_features, device=device)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = self.batch_norm(x)\n",
    "                return self.linear(x)\n",
    "\n",
    "        self.linear_layers = torch.nn.ModuleList([BatchNormLinear(self.all_dims[i], num_classes, device=\"cuda\") for i in range(len(self.all_dims))])\n",
    "\n",
    "    def prepare_input(self, x):  # preprocess\n",
    "        x = self.multichannel_fn(x)\n",
    "        x = F.interpolate(x, size=(224, 224), mode=\"bicubic\")\n",
    "        x = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406] * (x.shape[1] // 3), std=[0.229, 0.224, 0.225] * (x.shape[1] // 3))(x)\n",
    "        return x\n",
    "\n",
    "    def forward_until(self, x, layer_id):\n",
    "        x = self.prepare_input(x)\n",
    "        for l in range(layer_id):  # forward until layer_id\n",
    "            if list(x.shape)[1:] == [2048, 1, 1]:\n",
    "                x = x.reshape([-1, 2048])\n",
    "            x = self.layer_operations[l](x)\n",
    "        return x\n",
    "\n",
    "    def forward_after(self, x, layer_id):\n",
    "        x = self.prepare_input(x)\n",
    "        for l in range(layer_id, len(self.layer_operations)):  # forward from layer_id\n",
    "            if list(x.shape)[1:] == [2048, 1, 1]:\n",
    "                x = x.reshape([-1, 2048])\n",
    "            x = self.layer_operations[l](x)\n",
    "        return x\n",
    "\n",
    "    def predict_from_layer(self, x, l):\n",
    "        x = self.forward_until(x, l)\n",
    "        x = x.reshape([x.shape[0], -1])\n",
    "        return self.linear_layers[l](x)\n",
    "\n",
    "    def predict_from_several_layers(self, x, layers):\n",
    "        x = self.prepare_input(x)\n",
    "        outputs = dict()\n",
    "        outputs[0] = self.linear_layers[0](x.reshape([x.shape[0], -1]))\n",
    "        for l in range(len(self.layer_operations)):\n",
    "            if list(x.shape)[1:] == [2048, 1, 1]:\n",
    "                x = x.reshape([-1, 2048])\n",
    "            x = self.layer_operations[l](x)\n",
    "            if l in layers:\n",
    "                outputs[l + 1] = self.linear_layers[l + 1](x.reshape([x.shape[0], -1]))\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, model, layer_i):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.model = model\n",
    "        self.layer_i = layer_i\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.model.predict_from_layer(inputs, self.layer_i)\n",
    "\n",
    "\n",
    "resnet152_wrapper = WrapModelForResNet152(model.imported_model, make_multichannel_input, num_classes=num_classes)\n",
    "resnet152_wrapper.multichannel_fn = make_multichannel_input\n",
    "resnet152_wrapper = resnet152_wrapper.to(\"cuda\")\n",
    "# assert all([resnet152_wrapper.predict_from_layer(torch.Tensor(np.zeros((2, 3, 32, 32))).cuda(), layer_i).shape == torch.Size([2, 100]) for layer_i in range(53)])\n",
    "backbone_model = copy.deepcopy(resnet152_wrapper)\n",
    "del resnet152_wrapper\n",
    "\n",
    "backbone_model.eval()\n",
    "linear_model = LinearNet(backbone_model, 5).to(\"cuda\")  # just to have it ready\n",
    "\n",
    "layers_to_use = [20, 30, 35, 40, 45, 50, 52]  # only some layers to save time\n",
    "linear_layers_collected_dict = dict()\n",
    "\n",
    "for layer_i in reversed(layers_to_use):\n",
    "    print(f\"training layer={layer_i}\")\n",
    "    linear_model.layer_i = layer_i\n",
    "    linear_model.fixed_mode = \"train\"\n",
    "\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    robust_accs = []\n",
    "    clean_accs = []\n",
    "    actual_robust_accs = []\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    linear_model = train_model(\n",
    "        model_in=linear_model,\n",
    "        images_in=images_train_np[:],\n",
    "        labels_in=labels_train_np[:],\n",
    "        epochs=1,\n",
    "        lr=3.3e-5,\n",
    "        batch_size=64,\n",
    "        optimizer_in=optim.Adam,\n",
    "        subset_only=linear_model.model.linear_layers[layer_i].parameters(),  # just the linear projection\n",
    "        mode=\"train\",\n",
    "        use_adversarial_training=False,\n",
    "        adversarial_epsilon=8 / 255,\n",
    "        skip_test_set_eval=False,\n",
    "    )\n",
    "    linear_layers_collected_dict[layer_i] = copy.deepcopy(backbone_model.linear_layers[layer_i])\n",
    "\n",
    "# copy dict back to backbone\n",
    "for layer_i in layers_to_use:\n",
    "    backbone_model.linear_layers[layer_i] = copy.deepcopy(linear_layers_collected_dict[layer_i])\n",
    "del linear_layers_collected_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "import json\n",
    "\n",
    "#\n",
    "# eval test set\n",
    "#\n",
    "\n",
    "\n",
    "def eval_layer(model, images_in, labels_in, batch_size=128):\n",
    "    all_preds = []\n",
    "    all_logits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        its = int(np.ceil(float(len(images_in)) / float(batch_size)))\n",
    "        pbar = tqdm(range(its), desc=f\"layer eval\", ncols=100)\n",
    "        for it in pbar:\n",
    "            i1 = it * batch_size\n",
    "            i2 = min([(it + 1) * batch_size, len(images_in)])\n",
    "\n",
    "            inputs = torch.Tensor(images_in[i1:i2].transpose([0, 3, 1, 2])).to(\"cuda\")\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            all_logits.append(outputs.detach().cpu().numpy())\n",
    "            preds = torch.argmax(outputs, axis=-1)\n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_logits = np.concatenate(all_logits, axis=0)\n",
    "    return np.sum(all_preds == labels_in), all_preds.shape[0], all_logits\n",
    "\n",
    "\n",
    "def get_cross_max_consensus(outputs: torch.Tensor, k: int, self_assemble_mode: bool = False) -> torch.Tensor:\n",
    "    Z_hat = outputs - outputs.max(dim=2, keepdim=True)[0]  # subtract the max per-predictor over classes\n",
    "    Z_hat = Z_hat - Z_hat.max(dim=1, keepdim=True)[0]  # subtract the per-class max over predictors\n",
    "    Y, _ = torch.topk(Z_hat, k, dim=1)  # get highest k values per class\n",
    "    if self_assemble_mode:\n",
    "        Y = torch.median(Z_hat, dim=1)[0]  # get median value per class\n",
    "    else:\n",
    "        Y = Y[:, -1, :]  # get the k-th highest value per class\n",
    "    _, predicted = torch.max(Y, 1)  # get max val index\n",
    "    assert predicted.shape == (outputs.shape[0],)\n",
    "    assert len(predicted.shape) == 1\n",
    "    assert (predicted >= 0).all() and (predicted < outputs.shape[2]).all()\n",
    "    assert predicted.dtype == torch.int64\n",
    "    return predicted\n",
    "\n",
    "\n",
    "def get_cross_max_consensus_logits(outputs: torch.Tensor, k: int, self_assemble_mode: bool = False) -> torch.Tensor:\n",
    "    Z_hat = outputs - outputs.max(dim=2, keepdim=True)[0]\n",
    "    Z_hat = Z_hat - Z_hat.max(dim=1, keepdim=True)[0]\n",
    "    Y, _ = torch.topk(Z_hat, k, dim=1)\n",
    "    if self_assemble_mode:\n",
    "        Y = torch.median(Z_hat, dim=1)[0]\n",
    "    else:\n",
    "        Y = Y[:, -1, :]\n",
    "    # don't get maximum value, keep logits per class\n",
    "    assert Y.shape == (outputs.shape[0], outputs.shape[2])\n",
    "    assert len(Y.shape) == 2\n",
    "    return Y\n",
    "\n",
    "\n",
    "def eval_self_ensemble(backbone_model, images_test, labels_test, layers_to_use, batch_size=128):\n",
    "    backbone_model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(images_test), batch_size), desc=\"ensemble eval\", ncols=100):\n",
    "            # transpose the image to [batch_size, channels, height, width] format\n",
    "            batch_images = torch.Tensor(images_test[i : i + batch_size].transpose([0, 3, 1, 2])).cuda()\n",
    "            batch_labels = labels_test[i : i + batch_size]\n",
    "            layer_outputs = []\n",
    "            for layer_i in layers_to_use:\n",
    "                outputs = backbone_model.predict_from_layer(batch_images, layer_i)\n",
    "                layer_outputs.append(outputs.unsqueeze(1))\n",
    "            ensemble_outputs = torch.cat(layer_outputs, dim=1)  # [batch_size, num_layers, num_classes]\n",
    "\n",
    "            predictions = get_cross_max_consensus(ensemble_outputs, k=3, self_assemble_mode=True)\n",
    "\n",
    "            all_preds.append(predictions.cpu().numpy())\n",
    "            all_labels.append(batch_labels)\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    accuracy = np.mean(all_preds == all_labels)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "#\n",
    "# perturb test set\n",
    "#\n",
    "\n",
    "\n",
    "def ensemble_fgsm_attack(model, images, labels, epsilon, random_reps, batch_size):\n",
    "    model.eval()\n",
    "    perturbed_images = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(images), batch_size), desc=\"fgsm\", total=len(images) // batch_size, ncols=100):\n",
    "        batch_images = images[i:i + batch_size]\n",
    "        batch_labels = labels[i:i + batch_size]\n",
    "        \n",
    "        x = torch.FloatTensor(batch_images.transpose(0, 3, 1, 2)).cuda()\n",
    "        y = torch.LongTensor(batch_labels).cuda()\n",
    "        x.requires_grad = True\n",
    "        \n",
    "        for _ in range(random_reps):\n",
    "            free_mem()\n",
    "            layer_outputs = []\n",
    "            for layer_i in layers_to_use:\n",
    "                outputs = model.predict_from_layer(x, layer_i)\n",
    "                layer_outputs.append(outputs.unsqueeze(1))\n",
    "            ensemble_outputs = torch.cat(layer_outputs, dim=1)\n",
    "            logits = get_cross_max_consensus_logits(ensemble_outputs, k=3, self_assemble_mode=True)\n",
    "            loss = nn.CrossEntropyLoss()(logits, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            grad = x.grad.data\n",
    "            perturbed_x = x + epsilon * grad.sign()\n",
    "            perturbed_x = torch.clamp(perturbed_x, 0, 1)\n",
    "            \n",
    "            perturbed_images.append(perturbed_x.detach().cpu().numpy().transpose(0, 2, 3, 1))\n",
    "            \n",
    "            x.grad.zero_()\n",
    "    return np.concatenate(perturbed_images, axis=0)\n",
    "\n",
    "\n",
    "def last_layer_fgsm_attack(model, images, labels, epsilon, random_reps, batch_size):\n",
    "    model.eval()\n",
    "    perturbed_images = []\n",
    "    \n",
    "    last_layer = max(layers_to_use)\n",
    "    \n",
    "    for i in tqdm(range(0, len(images), batch_size), desc=\"fgsm\", total=len(images) // batch_size, ncols=100):\n",
    "        batch_images = images[i:i + batch_size]\n",
    "        batch_labels = labels[i:i + batch_size]\n",
    "        \n",
    "        x = torch.FloatTensor(batch_images.transpose(0, 3, 1, 2)).cuda()\n",
    "        y = torch.LongTensor(batch_labels).cuda()\n",
    "        x.requires_grad = True\n",
    "        \n",
    "        for _ in range(random_reps):\n",
    "            free_mem()\n",
    "            outputs = model.predict_from_layer(x, last_layer)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            grad = x.grad.data\n",
    "            perturbed_x = x + epsilon * grad.sign()\n",
    "            perturbed_x = torch.clamp(perturbed_x, 0, 1)\n",
    "            \n",
    "            perturbed_images.append(perturbed_x.detach().cpu().numpy().transpose(0, 2, 3, 1))\n",
    "            \n",
    "            x.grad.zero_()\n",
    "    return np.concatenate(perturbed_images, axis=0)\n",
    "\n",
    "\n",
    "def add_overlay(background: Image.Image, overlay: Image.Image, opacity: int) -> Image.Image:\n",
    "    # opacity range: 0 (transparent) to 255 (opaque)\n",
    "    overlay = overlay.resize(background.size)\n",
    "    result = Image.new(\"RGBA\", background.size)\n",
    "    result.paste(background, (0, 0))\n",
    "    mask = Image.new(\"L\", overlay.size, opacity)\n",
    "    result.paste(overlay, (0, 0), mask)\n",
    "    return result\n",
    "\n",
    "\n",
    "eval_mods = []\n",
    "free_mem()\n",
    "\n",
    "if False:\n",
    "    images_test_np = ensemble_fgsm_attack(backbone_model, images_test_np[:], labels_test_np[:], epsilon=8 / 255, random_reps=1, batch_size=64 // 2)\n",
    "    eval_mods.append(\"fgsm_ensemble\")\n",
    "\n",
    "if True:\n",
    "    images_test_np = last_layer_fgsm_attack(backbone_model, images_test_np[:], labels_test_np[:], epsilon=8 / 255, random_reps=1, batch_size=64 // 2)\n",
    "    eval_mods.append(\"fgsm_last_layer\")\n",
    "\n",
    "if False:\n",
    "    free_mem()\n",
    "    maskpath = get_current_dir().parent / \"data\" / \"mask.png\"\n",
    "    mask = Image.open(maskpath)\n",
    "    opacity = 128\n",
    "    for i in range(len(images_test_np)):\n",
    "        images_test_np[i] = add_overlay(Image.fromarray((255 * images_test_np[i]).astype(np.uint8)), mask, opacity).convert(\"RGB\")\n",
    "    eval_mods.append(f\"polygon_mask_opacity_{opacity}\")\n",
    "\n",
    "\n",
    "#\n",
    "# print results\n",
    "#\n",
    "\n",
    "\n",
    "backbone_model.cuda()\n",
    "backbone_model.eval()\n",
    "layer_acc = []\n",
    "for layer_i in layers_to_use:\n",
    "    linear_model = LinearNet(backbone_model, layer_i).to(\"cuda\")\n",
    "    linear_model.eval()\n",
    "    test_hits, test_count, _ = eval_layer(linear_model, images_test_np, labels_test_np)\n",
    "    layer_acc.append(test_hits / test_count)\n",
    "\n",
    "ensemble_acc = eval_self_ensemble(backbone_model, images_test_np, labels_test_np, layers_to_use)\n",
    "\n",
    "train_mods = []\n",
    "train_mods.append(\"noise\") if ENABLE_NOISE else None\n",
    "train_mods.append(\"shuffle\") if ENABLE_RANDOM_SHUFFLE else None\n",
    "train_mods.append(\"adv\") if ENABLE_ADVERSARIAL_TRAINING else None\n",
    "\n",
    "print(\"results:\")\n",
    "print(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"dataset\": dataset,\n",
    "            \"train_mods\": train_mods,\n",
    "            \"eval_mods\": eval_mods,\n",
    "            \"ensemble_acc\": ensemble_acc,\n",
    "            \"layer_acc\": {layer_i: acc for layer_i, acc in zip(layers_to_use, layer_acc)},\n",
    "        },\n",
    "        indent=4,\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
